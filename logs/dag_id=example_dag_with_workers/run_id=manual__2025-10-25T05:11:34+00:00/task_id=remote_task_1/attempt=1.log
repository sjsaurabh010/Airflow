[2025-10-25T05:11:44.057+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-10-25T05:11:44.081+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: example_dag_with_workers.remote_task_1 manual__2025-10-25T05:11:34+00:00 [queued]>
[2025-10-25T05:11:44.094+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: example_dag_with_workers.remote_task_1 manual__2025-10-25T05:11:34+00:00 [queued]>
[2025-10-25T05:11:44.094+0000] {taskinstance.py:2865} INFO - Starting attempt 1 of 1
[2025-10-25T05:11:44.107+0000] {taskinstance.py:2888} INFO - Executing <Task(SmartRemoteWorkerOperator): remote_task_1> on 2025-10-25 05:11:34+00:00
[2025-10-25T05:11:44.113+0000] {warnings.py:110} WARNING - /home/airflow/airflow-env/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=128768) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-10-25T05:11:44.114+0000] {standard_task_runner.py:72} INFO - Started process 128779 to run task
[2025-10-25T05:11:44.114+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'example_dag_with_workers', 'remote_task_1', 'manual__2025-10-25T05:11:34+00:00', '--job-id', '125', '--raw', '--subdir', 'DAGS_FOLDER/example_dag_with_workers.py', '--cfg-path', '/tmp/tmpwu7drtk8']
[2025-10-25T05:11:44.116+0000] {standard_task_runner.py:105} INFO - Job 125: Subtask remote_task_1
[2025-10-25T05:11:44.177+0000] {task_command.py:467} INFO - Running <TaskInstance: example_dag_with_workers.remote_task_1 manual__2025-10-25T05:11:34+00:00 [running]> on host masterdimpal-1
[2025-10-25T05:11:44.270+0000] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='example_dag_with_workers' AIRFLOW_CTX_TASK_ID='remote_task_1' AIRFLOW_CTX_EXECUTION_DATE='2025-10-25T05:11:34+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-10-25T05:11:34+00:00'
[2025-10-25T05:11:44.271+0000] {taskinstance.py:731} INFO - ::endgroup::
[2025-10-25T05:11:44.285+0000] {smart_remote_worker.py:506} INFO - ================================================================================
[2025-10-25T05:11:44.285+0000] {smart_remote_worker.py:507} INFO - üöÄ Smart Remote Worker Operator - Starting Execution
[2025-10-25T05:11:44.286+0000] {smart_remote_worker.py:508} INFO - ================================================================================
[2025-10-25T05:11:44.299+0000] {smart_remote_worker.py:120} INFO - üìã Loaded 1 workers from registry
[2025-10-25T05:11:44.300+0000] {smart_remote_worker.py:522} INFO - üìã Available workers (1):
[2025-10-25T05:11:44.300+0000] {smart_remote_worker.py:525} INFO -    - worker-1 (priority: 1, host: 45.151.155.74)
[2025-10-25T05:11:44.300+0000] {smart_remote_worker.py:529} INFO - ================================================================================
[2025-10-25T05:11:44.300+0000] {smart_remote_worker.py:530} INFO - üéØ ATTEMPT 1/1: Trying worker 'worker-1'
[2025-10-25T05:11:44.300+0000] {smart_remote_worker.py:531} INFO - ================================================================================
[2025-10-25T05:11:44.300+0000] {smart_remote_worker.py:163} INFO - üîç Testing SSH connection to airflow@45.151.155.74:22
[2025-10-25T05:11:44.310+0000] {transport.py:1944} INFO - Connected (version 2.0, client OpenSSH_9.6p1)
[2025-10-25T05:11:44.408+0000] {transport.py:1944} INFO - Authentication (publickey) successful!
[2025-10-25T05:11:44.822+0000] {smart_remote_worker.py:190} INFO - ‚úÖ SSH connection successful
[2025-10-25T05:11:44.822+0000] {smart_remote_worker.py:213} INFO - üöÄ Starting on-demand worker 'worker-1' on 45.151.155.74
[2025-10-25T05:11:44.823+0000] {smart_remote_worker.py:214} INFO -    Container type: singularity
[2025-10-25T05:11:44.823+0000] {smart_remote_worker.py:215} INFO -    Queue: default
[2025-10-25T05:11:44.834+0000] {transport.py:1944} INFO - Connected (version 2.0, client OpenSSH_9.6p1)
[2025-10-25T05:11:44.895+0000] {transport.py:1944} INFO - Authentication (publickey) successful!
[2025-10-25T05:11:44.896+0000] {smart_remote_worker.py:347} INFO - Executing worker start command...
[2025-10-25T05:11:44.896+0000] {smart_remote_worker.py:348} INFO - Command to execute:

#!/bin/bash
set -x  # Enable debug mode to see all commands
# set -e removed - we want to see all errors, not exit early

# First, try to detect AIRFLOW_HOME if not set
if [ -z "$AIRFLOW_HOME" ]; then
    # Try common locations
    if [ -d "/home/airflow/airflow-worker" ]; then
        export AIRFLOW_HOME="/home/airflow/airflow-worker"
    elif [ -d "/home/airflow/airflow-worker/Airflow" ]; then
        export AIRFLOW_HOME="/home/airflow/airflow-worker/Airflow"
    elif [ -d "/home/airflow/airflow" ]; then
        export AIRFLOW_HOME="/home/airflow/airflow"
    elif [ -d "/home/airflow/airflow" ]; then
        export AIRFLOW_HOME="/home/airflow/airflow"
    else
        export AIRFLOW_HOME="$HOME/airflow"
    fi
fi

echo "Using AIRFLOW_HOME: $AIRFLOW_HOME"

export WORKER_NAME="worker-1"
export QUEUE_NAME="default"
export MASTER_IP="45.151.155.100"

# Kill existing worker processes
pkill -f "celery.*worker.*worker-1" 2>/dev/null || true
sleep 2

# Setup SSH tunnels to master
echo "Setting up SSH tunnels to master 45.151.155.100..."
pkill -f "ssh.*-L.*5432" 2>/dev/null || true
pkill -f "ssh.*-L.*6379" 2>/dev/null || true

ssh -f -N \
    -o StrictHostKeyChecking=no \
    -o UserKnownHostsFile=/dev/null \
    -o ServerAliveInterval=60 \
    -o ServerAliveCountMax=3 \
    -L 5432:localhost:5432 \
    -L 6379:localhost:6379 \
    airflow@45.151.155.100

sleep 3

# Start Celery worker
echo "Starting Celery worker worker-1..."
cd $AIRFLOW_HOME

# Find Singularity image file
SINGULARITY_IMAGE=""
if [ -f "containers/airflow-worker.sif" ]; then
    SINGULARITY_IMAGE="containers/airflow-worker.sif"
elif [ -f "airflow-worker.sif" ]; then
    SINGULARITY_IMAGE="airflow-worker.sif"
elif [ -f "airflow_2.10.2.sif" ]; then
    SINGULARITY_IMAGE="airflow_2.10.2.sif"
elif [ -f "containers/airflow_2.10.2.sif" ]; then
    SINGULARITY_IMAGE="containers/airflow_2.10.2.sif"
fi

if [ -n "$SINGULARITY_IMAGE" ]; then
    # Start with Singularity (default)
    echo "Using Singularity container: $SINGULARITY_IMAGE"
    nohup singularity exec \
        --bind $AIRFLOW_HOME:$AIRFLOW_HOME \
        $SINGULARITY_IMAGE \
        celery -A airflow.providers.celery.executors.celery_executor.app worker \
        --hostname=$WORKER_NAME \
        --queues=$QUEUE_NAME,default \
        --concurrency=4 \
        --loglevel=info \
        > /tmp/worker_$WORKER_NAME.log 2>&1 &
else
    # Fallback to Python venv if Singularity image not found
    echo "Singularity image not found, using Python venv"
    if [ -f venv/bin/activate ]; then
        source venv/bin/activate
    fi
    
    nohup celery -A airflow.providers.celery.executors.celery_executor.app worker \
        --hostname=$WORKER_NAME \
        --queues=$QUEUE_NAME,default \
        --concurrency=4 \
        --loglevel=info \
        > /tmp/worker_$WORKER_NAME.log 2>&1 &
fi

sleep 5

# Verify worker started
if pgrep -f "celery.*worker.*worker-1" > /dev/null; then
    echo "‚úÖ Worker worker-1 started successfully"
    exit 0
else
    echo "‚ùå Worker worker-1 failed to start"
    tail -20 /tmp/worker_$WORKER_NAME.log 2>/dev/null || true
    exit 1
fi

[2025-10-25T05:11:45.005+0000] {smart_remote_worker.py:358} INFO - Exit status: -1
[2025-10-25T05:11:45.006+0000] {smart_remote_worker.py:360} INFO - STDOUT:
Using AIRFLOW_HOME: /home/airflow/airflow-worker

[2025-10-25T05:11:45.006+0000] {smart_remote_worker.py:362} INFO - STDERR:
+ '[' -z '' ']'
+ '[' -d /home/airflow/airflow-worker ']'
+ export AIRFLOW_HOME=/home/airflow/airflow-worker
+ AIRFLOW_HOME=/home/airflow/airflow-worker
+ echo 'Using AIRFLOW_HOME: /home/airflow/airflow-worker'
+ export WORKER_NAME=worker-1
+ WORKER_NAME=worker-1
+ export QUEUE_NAME=default
+ QUEUE_NAME=default
+ export MASTER_IP=45.151.155.100
+ MASTER_IP=45.151.155.100
+ pkill -f 'celery.*worker.*worker-1'

[2025-10-25T05:11:45.006+0000] {smart_remote_worker.py:365} ERROR - ‚ùå Worker start failed with exit code -1
[2025-10-25T05:11:45.006+0000] {smart_remote_worker.py:549} WARNING - ‚è≥ Worker start failed. Waiting 300s before retry...
[2025-10-25T05:16:45.007+0000] {smart_remote_worker.py:163} INFO - üîç Testing SSH connection to airflow@45.151.155.74:22
[2025-10-25T05:16:45.018+0000] {transport.py:1944} INFO - Connected (version 2.0, client OpenSSH_9.6p1)
[2025-10-25T05:16:45.076+0000] {transport.py:1944} INFO - Authentication (publickey) successful!
[2025-10-25T05:16:45.497+0000] {smart_remote_worker.py:190} INFO - ‚úÖ SSH connection successful
[2025-10-25T05:16:45.497+0000] {smart_remote_worker.py:213} INFO - üöÄ Starting on-demand worker 'worker-1' on 45.151.155.74
[2025-10-25T05:16:45.497+0000] {smart_remote_worker.py:214} INFO -    Container type: singularity
[2025-10-25T05:16:45.497+0000] {smart_remote_worker.py:215} INFO -    Queue: default
[2025-10-25T05:16:45.509+0000] {transport.py:1944} INFO - Connected (version 2.0, client OpenSSH_9.6p1)
[2025-10-25T05:16:45.607+0000] {transport.py:1944} INFO - Authentication (publickey) successful!
[2025-10-25T05:16:45.608+0000] {smart_remote_worker.py:347} INFO - Executing worker start command...
[2025-10-25T05:16:45.608+0000] {smart_remote_worker.py:348} INFO - Command to execute:

#!/bin/bash
set -x  # Enable debug mode to see all commands
# set -e removed - we want to see all errors, not exit early

# First, try to detect AIRFLOW_HOME if not set
if [ -z "$AIRFLOW_HOME" ]; then
    # Try common locations
    if [ -d "/home/airflow/airflow-worker" ]; then
        export AIRFLOW_HOME="/home/airflow/airflow-worker"
    elif [ -d "/home/airflow/airflow-worker/Airflow" ]; then
        export AIRFLOW_HOME="/home/airflow/airflow-worker/Airflow"
    elif [ -d "/home/airflow/airflow" ]; then
        export AIRFLOW_HOME="/home/airflow/airflow"
    elif [ -d "/home/airflow/airflow" ]; then
        export AIRFLOW_HOME="/home/airflow/airflow"
    else
        export AIRFLOW_HOME="$HOME/airflow"
    fi
fi

echo "Using AIRFLOW_HOME: $AIRFLOW_HOME"

export WORKER_NAME="worker-1"
export QUEUE_NAME="default"
export MASTER_IP="45.151.155.100"

# Kill existing worker processes
pkill -f "celery.*worker.*worker-1" 2>/dev/null || true
sleep 2

# Setup SSH tunnels to master
echo "Setting up SSH tunnels to master 45.151.155.100..."
pkill -f "ssh.*-L.*5432" 2>/dev/null || true
pkill -f "ssh.*-L.*6379" 2>/dev/null || true

ssh -f -N \
    -o StrictHostKeyChecking=no \
    -o UserKnownHostsFile=/dev/null \
    -o ServerAliveInterval=60 \
    -o ServerAliveCountMax=3 \
    -L 5432:localhost:5432 \
    -L 6379:localhost:6379 \
    airflow@45.151.155.100

sleep 3

# Start Celery worker
echo "Starting Celery worker worker-1..."
cd $AIRFLOW_HOME

# Find Singularity image file
SINGULARITY_IMAGE=""
if [ -f "containers/airflow-worker.sif" ]; then
    SINGULARITY_IMAGE="containers/airflow-worker.sif"
elif [ -f "airflow-worker.sif" ]; then
    SINGULARITY_IMAGE="airflow-worker.sif"
elif [ -f "airflow_2.10.2.sif" ]; then
    SINGULARITY_IMAGE="airflow_2.10.2.sif"
elif [ -f "containers/airflow_2.10.2.sif" ]; then
    SINGULARITY_IMAGE="containers/airflow_2.10.2.sif"
fi

if [ -n "$SINGULARITY_IMAGE" ]; then
    # Start with Singularity (default)
    echo "Using Singularity container: $SINGULARITY_IMAGE"
    nohup singularity exec \
        --bind $AIRFLOW_HOME:$AIRFLOW_HOME \
        $SINGULARITY_IMAGE \
        celery -A airflow.providers.celery.executors.celery_executor.app worker \
        --hostname=$WORKER_NAME \
        --queues=$QUEUE_NAME,default \
        --concurrency=4 \
        --loglevel=info \
        > /tmp/worker_$WORKER_NAME.log 2>&1 &
else
    # Fallback to Python venv if Singularity image not found
    echo "Singularity image not found, using Python venv"
    if [ -f venv/bin/activate ]; then
        source venv/bin/activate
    fi
    
    nohup celery -A airflow.providers.celery.executors.celery_executor.app worker \
        --hostname=$WORKER_NAME \
        --queues=$QUEUE_NAME,default \
        --concurrency=4 \
        --loglevel=info \
        > /tmp/worker_$WORKER_NAME.log 2>&1 &
fi

sleep 5

# Verify worker started
if pgrep -f "celery.*worker.*worker-1" > /dev/null; then
    echo "‚úÖ Worker worker-1 started successfully"
    exit 0
else
    echo "‚ùå Worker worker-1 failed to start"
    tail -20 /tmp/worker_$WORKER_NAME.log 2>/dev/null || true
    exit 1
fi

[2025-10-25T05:16:45.716+0000] {smart_remote_worker.py:358} INFO - Exit status: -1
[2025-10-25T05:16:45.716+0000] {smart_remote_worker.py:360} INFO - STDOUT:
Using AIRFLOW_HOME: /home/airflow/airflow-worker

[2025-10-25T05:16:45.716+0000] {smart_remote_worker.py:362} INFO - STDERR:
+ '[' -z '' ']'
+ '[' -d /home/airflow/airflow-worker ']'
+ export AIRFLOW_HOME=/home/airflow/airflow-worker
+ AIRFLOW_HOME=/home/airflow/airflow-worker
+ echo 'Using AIRFLOW_HOME: /home/airflow/airflow-worker'
+ export WORKER_NAME=worker-1
+ WORKER_NAME=worker-1
+ export QUEUE_NAME=default
+ QUEUE_NAME=default
+ export MASTER_IP=45.151.155.100
+ MASTER_IP=45.151.155.100
+ pkill -f 'celery.*worker.*worker-1'

[2025-10-25T05:16:45.717+0000] {smart_remote_worker.py:365} ERROR - ‚ùå Worker start failed with exit code -1
[2025-10-25T05:16:45.717+0000] {smart_remote_worker.py:553} ERROR - ‚ùå Failed to start worker 'worker-1' after retry
[2025-10-25T05:16:45.717+0000] {smart_remote_worker.py:590} ERROR - ================================================================================
[2025-10-25T05:16:45.717+0000] {smart_remote_worker.py:591} ERROR - ‚ùå FAILURE: All workers failed or unavailable
[2025-10-25T05:16:45.717+0000] {smart_remote_worker.py:592} ERROR - ================================================================================
[2025-10-25T05:16:45.732+0000] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/airflow-env/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/airflow-env/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/airflow-env/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/airflow-env/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/airflow/dags/../plugins/operators/smart_remote_worker.py", line 593, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Task failed on all available workers. Tried 1 worker(s): ['worker-1']
[2025-10-25T05:16:45.737+0000] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=example_dag_with_workers, task_id=remote_task_1, run_id=manual__2025-10-25T05:11:34+00:00, execution_date=20251025T051134, start_date=20251025T051144, end_date=20251025T051645
[2025-10-25T05:16:45.749+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2025-10-25T05:16:45.749+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 125 for task remote_task_1 (Task failed on all available workers. Tried 1 worker(s): ['worker-1']; 128779)
Traceback (most recent call last):
  File "/home/airflow/airflow-env/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/airflow-env/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/airflow-env/lib/python3.12/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/airflow-env/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/airflow-env/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/airflow-env/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/airflow-env/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/airflow-env/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/airflow-env/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/airflow-env/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/airflow-env/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/airflow-env/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/airflow-env/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/airflow-env/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/airflow-env/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/airflow/dags/../plugins/operators/smart_remote_worker.py", line 593, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Task failed on all available workers. Tried 1 worker(s): ['worker-1']
[2025-10-25T05:16:45.786+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-10-25T05:16:45.814+0000] {taskinstance.py:3900} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-10-25T05:16:45.818+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
