<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Airflow Smart Remote Worker Setup Guide</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }
        .container {
            background: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #017cee;
            border-bottom: 3px solid #017cee;
            padding-bottom: 10px;
        }
        h2 {
            color: #333;
            margin-top: 30px;
            background: #f0f8ff;
            padding: 10px;
            border-left: 4px solid #017cee;
        }
        h3 {
            color: #555;
            margin-top: 20px;
        }
        .feature-box {
            background: #e8f5e9;
            padding: 15px;
            margin: 15px 0;
            border-radius: 5px;
            border-left: 4px solid #4caf50;
        }
        .warning-box {
            background: #fff3cd;
            padding: 15px;
            margin: 15px 0;
            border-radius: 5px;
            border-left: 4px solid #ffc107;
        }
        .info-box {
            background: #e3f2fd;
            padding: 15px;
            margin: 15px 0;
            border-radius: 5px;
            border-left: 4px solid #2196f3;
        }
        .file-structure {
            background: #1e1e1e;
            color: #d4d4d4;
            padding: 20px;
            border-radius: 8px;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            overflow-x: auto;
            line-height: 2;
            white-space: pre;
            font-size: 14px;
            border: 1px solid #333;
            position: relative;
            margin: 20px 0;
        }
        .file-structure::before {
            content: 'üìÅ File Structure';
            position: absolute;
            top: -10px;
            left: 15px;
            background: #1e1e1e;
            padding: 0 10px;
            color: #4ec9b0;
            font-family: 'Segoe UI', sans-serif;
            font-size: 12px;
            font-weight: bold;
        }
        .step {
            background: #fafafa;
            padding: 15px;
            margin: 10px 0;
            border-radius: 5px;
            border: 1px solid #ddd;
        }
        .step-number {
            background: #017cee;
            color: white;
            padding: 5px 12px;
            border-radius: 50%;
            font-weight: bold;
            margin-right: 10px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th {
            background: #017cee;
            color: white;
            padding: 12px;
            text-align: left;
        }
        td {
            padding: 10px;
            border-bottom: 1px solid #ddd;
        }
        tr:hover {
            background: #f5f5f5;
        }
        .flow-diagram {
            background: white;
            padding: 20px;
            margin: 20px 0;
            border: 2px solid #017cee;
            border-radius: 5px;
            text-align: center;
        }
        .arrow {
            color: #017cee;
            font-size: 24px;
            margin: 10px 0;
        }
        code {
            background: #f4f4f4;
            padding: 3px 8px;
            border-radius: 4px;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            color: #d63384;
            font-size: 14px;
            border: 1px solid #e0e0e0;
        }
        .command {
            background: #282c34;
            color: #abb2bf;
            padding: 20px;
            padding-top: 45px;
            border-radius: 8px;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            overflow-x: auto;
            margin: 15px 0;
            white-space: pre-wrap;
            word-wrap: break-word;
            line-height: 1.8;
            font-size: 14px;
            border: 1px solid #3e4451;
            position: relative;
        }
        .command::before {
            content: '$ Terminal';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            background: #21252b;
            padding: 8px 15px;
            color: #61afef;
            font-size: 12px;
            font-weight: bold;
            border-bottom: 1px solid #3e4451;
            border-radius: 8px 8px 0 0;
        }
        .copy-btn {
            position: absolute;
            top: 8px;
            right: 10px;
            background: #4ec9b0;
            color: #1e1e1e;
            border: none;
            padding: 5px 12px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 11px;
            font-weight: bold;
            font-family: 'Segoe UI', sans-serif;
            transition: all 0.2s;
            z-index: 10;
        }
        .copy-btn:hover {
            background: #6ee7b7;
            transform: scale(1.05);
        }
        .copy-btn:active {
            transform: scale(0.95);
        }
        .copy-btn.copied {
            background: #22c55e;
            color: white;
        }
        ul {
            line-height: 2;
        }
        .highlight {
            background: yellow;
            padding: 2px 4px;
            font-weight: bold;
        }
    </style>
    <script>
        function copyToClipboard(button) {
            const codeBlock = button.parentElement;
            const textContent = codeBlock.textContent.replace('Copy', '').replace('Copied!', '').trim();
            
            navigator.clipboard.writeText(textContent).then(() => {
                button.textContent = 'Copied!';
                button.classList.add('copied');
                setTimeout(() => {
                    button.textContent = 'Copy';
                    button.classList.remove('copied');
                }, 2000);
            }).catch(err => {
                console.error('Failed to copy:', err);
            });
        }
    </script>
</head>
<body>
    
    <div class="content">
        <!-- OVERVIEW SECTION -->
        <section id="overview" class="section">
            <h2><span class="icon">üìã</span> Overview</h2>
            
            <div class="important-box">
                <h3>üéØ Core Concepts</h3>
                <ul style="margin-left: 20px;">
                    <li><strong>Master Node:</strong> Runs all services (PostgreSQL, Redis, Scheduler, Webserver)</li>
                    <li><strong>Worker Nodes:</strong> Ephemeral, no services, connect via SSH tunnels only</li>
                    <li><strong>On-Demand:</strong> Workers start, execute tasks, then stop immediately</li>
                    <li><strong>No Root Access:</strong> Everything runs as airflow user</li>
                    <li><strong>SSH Tunnels:</strong> All worker-to-master communication via SSH port forwarding</li>
                    <li><strong>Container-Based:</strong> Tasks run in Docker or Singularity containers</li>
                </ul>
            </div>

            <div class="info">
                <strong>Implementation Details:</strong>
                <ul style="margin-left: 20px;">
                    <li>üîß Airflow Version: <strong>2.10.2</strong> (Latest stable)</li>
                    <li>üì¶ Container: <strong>Singularity only</strong> (built on master, deployed via SCP)</li>
                    <li>üìç Master Location: <code>/home/airflow/airflow</code></li>
                    <li>üóÑÔ∏è PostgreSQL database on master (port 5432)</li>
                    <li>üì° Redis broker on master (port 6379)</li>
                    <li>‚öôÔ∏è CeleryExecutor for distributed tasks</li>
                    <li>üîí SSH tunnels for all worker connections</li>
                </ul>
            </div>
        </section>

        <!-- ARCHITECTURE SECTION -->
        <section id="architecture" class="section">
            <h2><span class="icon">üèóÔ∏è</span> System Architecture</h2>
            
            <div class="architecture-diagram">
                <pre>
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        MASTER NODE                          ‚îÇ
‚îÇ                  /home/airflow/airflow                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ  PostgreSQL  ‚îÇ  ‚îÇ    Redis     ‚îÇ  ‚îÇ   Scheduler  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ    (5432)    ‚îÇ  ‚îÇ    (6379)    ‚îÇ  ‚îÇ              ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ  Webserver   ‚îÇ  ‚îÇ   Flower     ‚îÇ  ‚îÇ     DAGs     ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ    (8080)    ‚îÇ  ‚îÇ    (5555)    ‚îÇ  ‚îÇ   (Git Repo) ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚îÇ SSH Tunnel (Port 22 Only)
                      ‚îÇ No Direct Network Access
                      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              WORKER NODES                    ‚îÇ
‚îÇ         (On-Demand, No Root Access)          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                               ‚îÇ
‚îÇ  ‚ùå NO PostgreSQL   ‚ùå NO Redis              ‚îÇ
‚îÇ  ‚ùå NO Persistent Services                    ‚îÇ
‚îÇ                                               ‚îÇ
‚îÇ  ‚úÖ SSH Client                               ‚îÇ
‚îÇ  ‚úÖ Python venv  / Singularity               ‚îÇ
‚îÇ  ‚úÖ Celery Worker (Temporary)                ‚îÇ
‚îÇ  ‚úÖ Git for DAG Sync (Optional)              ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  Connects to Master via:                     ‚îÇ
‚îÇ  localhost:5432 ‚îÄ‚îÄSSH‚îÄ‚îÄ> Master:5432         ‚îÇ
‚îÇ  localhost:6379 ‚îÄ‚îÄSSH‚îÄ‚îÄ> Master:6379         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                </pre>
            </div>

            <h3>Communication Flow</h3>
            <table>
                <thead>
                    <tr>
                        <th>Step</th>
                        <th>Action</th>
                        <th>Direction</th>
                        <th>Protocol</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>1</td>
                        <td>Master triggers task</td>
                        <td>Master ‚Üí Queue</td>
                        <td>Redis</td>
                    </tr>
                    <tr>
                        <td>2</td>
                        <td>SSH to worker</td>
                        <td>Master ‚Üí Worker</td>
                        <td>SSH</td>
                    </tr>
                    <tr>
                        <td>3</td>
                        <td>Setup SSH tunnel</td>
                        <td>Worker ‚Üí Master</td>
                        <td>SSH</td>
                    </tr>
                    <tr>
                        <td>4</td>
                        <td>Start Celery worker</td>
                        <td>Worker</td>
                        <td>Local</td>
                    </tr>
                    <tr>
                        <td>5</td>
                        <td>Pull task from queue</td>
                        <td>Worker ‚Üí Master</td>
                        <td>Redis via tunnel</td>
                    </tr>
                    <tr>
                        <td>6</td>
                        <td>Execute task</td>
                        <td>Worker</td>
                        <td>Container</td>
                    </tr>
                    <tr>
                        <td>7</td>
                        <td>Update status</td>
                        <td>Worker ‚Üí Master</td>
                        <td>PostgreSQL via tunnel</td>
                    </tr>
                    <tr>
                        <td>8</td>
                        <td>Stop worker</td>
                        <td>Worker</td>
                        <td>Local</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- PREREQUISITES SECTION -->
        <section id="prerequisites" class="section">
            <h2><span class="icon">‚úÖ</span> Prerequisites</h2>
            
            <div class="grid">
                <div class="card">
                    <h4>Master Node Requirements</h4>
                    <ul>
                        <li>Ubuntu 20.04+ or CentOS 8+</li>
                        <li>4+ CPU cores, 8GB+ RAM</li>
                        <li>Python 3.8+</li>
                        <li>PostgreSQL 12+</li>
                        <li>Redis 6+</li>
                        <li>Git access to DAG repository</li>
                    </ul>
                </div>
                <div class="card">
                    <h4>Worker Node Requirements</h4>
                    <ul>
                        <li>Ubuntu 20.04+ or CentOS 8+</li>
                        <li>2+ CPU cores, 4GB+ RAM</li>
                        <li>Python 3.8+</li>
                        <li>SSH server running</li>
                        <li>Docker or Singularity (optional)</li>
                        <li>No root access required!</li>
                    </ul>
                </div>
            </div>


            <div class="container">
        <h1>üöÄ Airflow Smart Remote Worker Setup Guide</h1>
        <p><strong>Purpose:</strong> Complete process to add and manage remote workers with SSH-based architecture</p>
        
        <div class="feature-box">
            <h3>‚ú® 7 Core Features</h3>
            <ul>
                <li><strong>No Hardcoded IPs:</strong> Workers defined in Airflow Variables (dynamic)</li>
                <li><strong>Easy Scaling:</strong> Add workers via UI/CLI without code changes</li>
                <li><strong>Priority-Based Selection:</strong> Each DAG specifies its worker preference</li>
                <li><strong>On-Demand Execution:</strong> Workers start only when needed</li>
                <li><strong>SSH-Only Access:</strong> All connections via SSH tunnels</li>
                <li><strong>5-Minute Retry:</strong> Built-in retry logic with failover</li>
                <li><strong>No Root Required:</strong> Works with regular user permissions</li>
            </ul>
        </div>

        <!-- ===== FILE STRUCTURE ===== -->
        <h2>üìÅ Complete File Structure</h2>
        <div class="file-structure">
/home/airflow/airflow/
‚îú‚îÄ‚îÄ plugins/
‚îÇ   ‚îî‚îÄ‚îÄ operators/
‚îÇ       ‚îî‚îÄ‚îÄ smart_remote_worker.py           # Main operator (NO hardcoded IPs)
‚îÇ
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îú‚îÄ‚îÄ worker_health_monitor.py             # Checks worker availability every 5 min
‚îÇ   ‚îî‚îÄ‚îÄ example_dag_with_workers.py          # Sample DAG with priority list
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ manage_workers.py                    # CLI to add/remove workers
‚îÇ   ‚îú‚îÄ‚îÄ worker_launcher.sh                   # Script deployed to remote workers
‚îÇ   ‚îî‚îÄ‚îÄ setup_ssh_keys.sh                    # SSH key setup helper
‚îÇ
‚îî‚îÄ‚îÄ config/
    ‚îî‚îÄ‚îÄ worker_templates.json                # Optional: SSH connection templates
        </div>

        <h3>üìÑ File Purposes</h3>
        <table>
            <tr>
                <th>File</th>
                <th>Purpose</th>
                <th>Used By</th>
            </tr>
            <tr>
                <td><code>smart_remote_worker.py</code></td>
                <td>Custom operator that reads worker registry, selects based on priority, establishes SSH tunnels, manages on-demand lifecycle</td>
                <td>DAGs</td>
            </tr>
            <tr>
                <td><code>worker_health_monitor.py</code></td>
                <td>Scheduled DAG that checks SSH connectivity to all registered workers every 5 minutes, updates status in registry</td>
                <td>Airflow Scheduler</td>
            </tr>
            <tr>
                <td><code>example_dag_with_workers.py</code></td>
                <td>Sample DAG demonstrating how to specify worker priority list</td>
                <td>User Reference</td>
            </tr>
            <tr>
                <td><code>manage_workers.py</code></td>
                <td>Command-line tool to add/remove/list workers in registry without touching code</td>
                <td>Admin</td>
            </tr>
            <tr>
                <td><code>worker_launcher.sh</code></td>
                <td>Bash script copied to remote workers that starts Celery worker in container with SSH tunnels</td>
                <td>Remote Workers</td>
            </tr>
            <tr>
                <td><code>setup_ssh_keys.sh</code></td>
                <td>Helper script to generate and copy SSH keys to remote workers</td>
                <td>Admin (one-time setup)</td>
            </tr>
            <tr>
                <td><code>worker_templates.json</code></td>
                <td>Optional template file with common SSH configurations</td>
                <td>manage_workers.py</td>
            </tr>
        </table>

        <!-- ===== ARCHITECTURE OVERVIEW ===== -->
        <h2>üèóÔ∏è System Architecture</h2>
        
        <div class="flow-diagram">
            <h3>Master Node Components</h3>
            <div style="background: #e3f2fd; padding: 15px; margin: 10px; border-radius: 5px;">
                <strong>Airflow Webserver</strong><br>
                UI for monitoring and management
            </div>
            <div class="arrow">‚Üï</div>
            <div style="background: #e3f2fd; padding: 15px; margin: 10px; border-radius: 5px;">
                <strong>Airflow Scheduler</strong><br>
                Triggers DAGs and monitors execution
            </div>
            <div class="arrow">‚Üï</div>
            <div style="background: #e3f2fd; padding: 15px; margin: 10px; border-radius: 5px;">
                <strong>PostgreSQL Database</strong><br>
                Stores metadata + Worker Registry (Airflow Variables)
            </div>
            <div class="arrow">‚Üï</div>
            <div style="background: #e3f2fd; padding: 15px; margin: 10px; border-radius: 5px;">
                <strong>Message Broker (Redis/RabbitMQ)</strong><br>
                Celery task queue
            </div>
            <div class="arrow">‚Üì (SSH Tunnel)</div>
            <div style="background: #c8e6c9; padding: 15px; margin: 10px; border-radius: 5px;">
                <strong>Remote Worker Node</strong><br>
                On-demand Celery worker in container
            </div>
        </div>

        <div class="info-box">
            <strong>Key Point:</strong> All communication from remote workers back to master happens through SSH tunnels. Workers don't need direct network access to PostgreSQL or message broker.
        </div>

        <!-- ===== WORKER REGISTRY ===== -->
        <h2>üìä Worker Registry Structure</h2>
        <p>The worker registry is stored in <strong>Airflow Variables</strong> (in PostgreSQL), accessible via UI or CLI.</p>
        
        <h3>Registry JSON Schema</h3>
        <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
{
  "workers": {
    "worker-1": {
      "host": "45.151.155.74",
      "ssh_port": 22,
      "ssh_user": "airflow",
      "priority": 1,
      "status": "available",
      "container_type": "singularity",
      "tags": ["gpu", "high-memory"]
    },
    "worker-2": {
      "host": "10.0.1.20",
      "ssh_port": 22,
      "ssh_user": "airflow",
      "priority": 2,
      "status": "available",
      "container_type": "singularity",
      "tags": ["cpu-only"]
    }
  },
  "last_updated": "2024-10-25T10:30:00Z"
}
        </div>

        <h3>Field Descriptions</h3>
        <table>
            <tr>
                <th>Field</th>
                <th>Description</th>
                <th>Example</th>
            </tr>
            <tr>
                <td><code>host</code></td>
                <td>Remote worker IP address or hostname</td>
                <td>45.151.155.74</td>
            </tr>
            <tr>
                <td><code>ssh_port</code></td>
                <td>SSH port on remote worker</td>
                <td>22</td>
            </tr>
            <tr>
                <td><code>ssh_user</code></td>
                <td>Username on remote worker</td>
                <td>airflow</td>
            </tr>
            <tr>
                <td><code>priority</code></td>
                <td>Global priority (lower = higher priority)</td>
                <td>1</td>
            </tr>
            <tr>
                <td><code>status</code></td>
                <td>Current availability (updated by health monitor)</td>
                <td>available / unavailable</td>
            </tr>
            <tr>
                <td><code>container_type</code></td>
                <td>Container runtime on worker</td>
                <td>singularity / venv</td>
            </tr>
            <tr>
                <td><code>tags</code></td>
                <td>Optional labels for filtering</td>
                <td>["gpu", "high-memory"]</td>
            </tr>
        </table>

        <!-- ===== SETUP PROCESS ===== -->
        <h2>üîß Complete Setup Process</h2>

        <div class="warning-box">
            <h3>‚ö†Ô∏è Important: Singularity Workflow</h3>
            <p><strong>This implementation uses Singularity (not Docker)</strong></p>
            <p>You must build the Singularity image on the <strong>master server</strong> (where you have root access), then copy it to remote workers via SCP.</p>
            <p><strong>Prerequisites:</strong></p>
            <ul>
                <li>Singularity installed on master server (requires root for installation)</li>
                <li>Build image once on master: <code>sudo singularity build airflow_2.10.2.sif airflow.def</code></li>
                <li>Copy to all workers: <code>scp airflow_2.10.2.sif worker:/home/airflow/airflow/</code></li>
                <li>Workers can run the image without root access</li>
            </ul>
            <p>See <strong>SINGULARITY_SETUP_GUIDE.md</strong> for detailed instructions.</p>
        </div>

        <!-- STEP 0: Build Singularity Image -->
        <div class="step" style="background: #fff3cd; border-left: 4px solid #ffc107;">
            <h3><span class="step-number">0</span>Build Singularity Image (On Master Server with Root)</h3>
            <strong>What this does:</strong> Creates the Airflow Singularity container image
            <br><strong>Where to do it:</strong> Master server (where you have root/sudo access)
            <br><br><strong>How to do it:</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Create Singularity definition file
cd /home/airflow/airflow
cat > airflow.def << 'EOF'
Bootstrap: docker
From: apache/airflow:2.10.2

%post
    pip install --no-cache-dir psycopg2-binary redis celery

%environment
    export AIRFLOW_HOME=/home/airflow/airflow

%runscript
    exec celery -A airflow.providers.celery.executors.celery_executor.app worker "$@"
EOF

# Build the image (REQUIRES ROOT)
sudo singularity build airflow_2.10.2.sif airflow.def

# Change ownership so airflow user can copy it
sudo chown airflow:airflow airflow_2.10.2.sif

# Verify the build
singularity exec airflow_2.10.2.sif airflow version
            </div>
            <strong>What you get:</strong>
            <ul>
                <li>File: <code>airflow_2.10.2.sif</code> (~1-2 GB)</li>
                <li>Contains Airflow 2.10.2 with all dependencies</li>
                <li>Ready to copy to remote workers</li>
            </ul>
            <div class="info-box">
                <strong>üìù Note:</strong> This step only needs to be done once on the master server. The resulting .sif file will be copied to all workers.
            </div>
        </div>

        <!-- STEP 0.5: Copy Image to Workers -->
        <div class="step" style="background: #e8f5e9; border-left: 4px solid #4caf50;">
            <h3><span class="step-number">¬Ω</span>Copy Singularity Image to Remote Workers</h3>
            <strong>What this does:</strong> Distributes the Singularity image to all remote workers
            <br><strong>Prerequisites:</strong> SSH keys must be setup (see Step 2)
            <br><br><strong>Using automated script:</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Copy to single worker
./scripts/copy_singularity_to_workers.sh 45.151.155.74

# Copy to multiple workers at once
./scripts/copy_singularity_to_workers.sh 45.151.155.74 10.0.1.20 192.168.1.100
            </div>
            <strong>Or manually:</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Copy the .sif file via SCP
scp -i ~/.ssh/airflow_worker_key \
    airflow_2.10.2.sif \
    airflow@45.151.155.74:/home/airflow/airflow/

# Verify on remote
ssh -i ~/.ssh/airflow_worker_key airflow@45.151.155.74 \
    "ls -lh /home/airflow/airflow/airflow_2.10.2.sif"

# Test execution on remote (no root needed!)
ssh -i ~/.ssh/airflow_worker_key airflow@45.151.155.74 \
    "singularity exec /home/airflow/airflow/airflow_2.10.2.sif python --version"
            </div>
            <div class="warning-box">
                <strong>‚ö†Ô∏è Important:</strong> Copy this file to ALL workers before proceeding. The file is large (~1-2 GB) so copying may take several minutes per worker.
            </div>
        </div>

        <!-- STEP 0.75: Remote Worker Directory Structure -->
        <div class="step" style="background: #e3f2fd; border-left: 4px solid #2196f3;">
            <h3><span class="step-number">üìÅ</span>Remote Worker Directory Structure & File Locations</h3>
            <strong>Important:</strong> Understanding where to place files on remote workers
            <br><br>
            
            <h4>Typical Remote Worker Structure:</h4>
            <div class="file-structure">
/home/airflow/                              (Home directory)
‚îú‚îÄ‚îÄ worker_launcher.sh                      ‚Üê Script goes HERE (executable)
‚îÇ
‚îî‚îÄ‚îÄ airflow-worker/                         (Or "airflow" or custom name)
    ‚îî‚îÄ‚îÄ Airflow/                            (AIRFLOW_HOME directory)
        ‚îú‚îÄ‚îÄ airflow_2.10.2.sif             ‚Üê Singularity image HERE
        ‚îú‚îÄ‚îÄ dags/                           (DAG files - synced)
        ‚îú‚îÄ‚îÄ logs/                           (Worker logs)
        ‚îú‚îÄ‚îÄ config/                         (Optional configs)
        ‚îî‚îÄ‚îÄ venv/                           (Python venv - fallback)
            </div>

            <h4>Step 1: Identify Your Airflow Directory on Worker</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Login to remote worker
ssh airflow@45.151.155.74

# Check current directory structure
pwd
ls -la

# Common locations:
# - /home/airflow/airflow/
# - /home/airflow/airflow-worker/Airflow/
# - /opt/airflow/
            </div>

            <h4>Step 2: Set AIRFLOW_HOME Environment Variable</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Example: If your Airflow is in ~/airflow-worker/Airflow/
export AIRFLOW_HOME=/home/airflow/airflow-worker/Airflow

# Make it permanent (add to ~/.bashrc)
echo 'export AIRFLOW_HOME=/home/airflow/airflow-worker/Airflow' >> ~/.bashrc
source ~/.bashrc

# Verify
echo $AIRFLOW_HOME
            </div>

            <h4>Step 3: Place Files in Correct Locations</h4>
            <strong>A. Worker Launcher Script ‚Üí HOME directory</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Script location: /home/airflow/worker_launcher.sh
cd ~
ls -l worker_launcher.sh

# If copying manually from master:
scp -i ~/.ssh/airflow_worker_key \
    master:/path/to/worker_launcher.sh \
    ~/worker_launcher.sh

# Make executable
chmod +x ~/worker_launcher.sh
            </div>

            <strong>B. Singularity Image ‚Üí AIRFLOW_HOME directory</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Image location: $AIRFLOW_HOME/airflow_2.10.2.sif
# Example: /home/airflow/airflow-worker/Airflow/airflow_2.10.2.sif

# Create directory if needed
mkdir -p $AIRFLOW_HOME

# Copy from master
scp -i ~/.ssh/airflow_worker_key \
    master:/home/airflow/airflow/airflow_2.10.2.sif \
    $AIRFLOW_HOME/

# Verify
ls -lh $AIRFLOW_HOME/airflow_2.10.2.sif
            </div>

            <h4>Step 4: Verification Checklist</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# 1. Check AIRFLOW_HOME is set
echo $AIRFLOW_HOME
# Should show: /home/airflow/airflow-worker/Airflow (or your path)

# 2. Check worker_launcher.sh exists and is executable
ls -l ~/worker_launcher.sh
# Should show: -rwxr-xr-x

# 3. Check Singularity image exists
ls -lh $AIRFLOW_HOME/airflow_2.10.2.sif
# Should show: ~1-2GB file

# 4. Test Singularity works
singularity --version
singularity exec $AIRFLOW_HOME/airflow_2.10.2.sif python --version

# 5. Verify directory structure
tree -L 2 /home/airflow/
# Or: ls -R /home/airflow/
            </div>

            <div class="info-box">
                <strong>üìù Common Worker Directory Names:</strong>
                <ul>
                    <li><code>/home/airflow/airflow/</code> - Standard setup</li>
                    <li><code>/home/airflow/airflow-worker/Airflow/</code> - Your current structure</li>
                    <li><code>/opt/airflow/</code> - System-wide installation</li>
                    <li><code>/home/airflow/airflow_home/</code> - Custom name</li>
                </ul>
                <strong>Key Point:</strong> The <code>worker_launcher.sh</code> script will use <code>$AIRFLOW_HOME</code> to find the Singularity image, so make sure this environment variable is set correctly!
            </div>

            <div class="warning-box">
                <strong>‚ö†Ô∏è Important Notes:</strong>
                <ul>
                    <li><strong>Worker Script Location:</strong> MUST be in home directory (<code>~/worker_launcher.sh</code>)</li>
                    <li><strong>Singularity Image:</strong> MUST be in <code>$AIRFLOW_HOME/</code> directory</li>
                    <li><strong>AIRFLOW_HOME:</strong> MUST be set as environment variable (add to <code>~/.bashrc</code>)</li>
                    <li><strong>Permissions:</strong> Worker script must be executable (<code>chmod +x</code>)</li>
                </ul>
            </div>

            <h4>Example: Complete Setup on Worker with Custom Structure</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Your worker structure: /home/airflow/airflow-worker/Airflow/

# 1. Set AIRFLOW_HOME
export AIRFLOW_HOME=/home/airflow/airflow-worker/Airflow
echo 'export AIRFLOW_HOME=/home/airflow/airflow-worker/Airflow' >> ~/.bashrc

# 2. Create directories if needed
mkdir -p $AIRFLOW_HOME

# 3. Place worker_launcher.sh in home
cd ~
# (Master will SCP it here)

# 4. Copy Singularity image to AIRFLOW_HOME
# (Master will SCP it to: $AIRFLOW_HOME/airflow_2.10.2.sif)

# 5. Final structure:
# /home/airflow/
# ‚îú‚îÄ‚îÄ worker_launcher.sh              ‚Üê Here
# ‚îî‚îÄ‚îÄ airflow-worker/
#     ‚îî‚îÄ‚îÄ Airflow/
#         ‚îî‚îÄ‚îÄ airflow_2.10.2.sif      ‚Üê Here

# 6. Verify everything
echo "AIRFLOW_HOME: $AIRFLOW_HOME"
ls -l ~/worker_launcher.sh
ls -lh $AIRFLOW_HOME/airflow_2.10.2.sif
            </div>
        </div>

        <!-- STEP 1 -->
        <div class="step">
            <h3><span class="step-number">1</span>Master Node Prerequisites</h3>
            <strong>What to verify:</strong>
            <ul>
                <li>Airflow installed with CeleryExecutor configured</li>
                <li>PostgreSQL database running and accessible</li>
                <li>Message broker (Redis or RabbitMQ) running</li>
                <li>SSH client installed on master node</li>
                <li>Airflow user has <code>.ssh</code> directory</li>
            </ul>
            <strong>How to check:</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Check Airflow executor
airflow config get-value core executor
# Should show: CeleryExecutor

# Check PostgreSQL connection
psql -h localhost -U airflow -d airflow_db -c "\conninfo"

# Check Redis/RabbitMQ
redis-cli ping    # For Redis
rabbitmqctl status # For RabbitMQ

# Check SSH directory
ls -la /home/airflow/.ssh/
            </div>
        </div>

        <!-- STEP 2 -->
        <div class="step">
            <h3><span class="step-number">2</span>Generate SSH Keys on Master</h3>
            <strong>What this does:</strong> Creates SSH key pair for passwordless authentication to remote workers
            <br><strong>How to do it:</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Run as airflow user on master node
cd /home/airflow/.ssh/

# Generate new SSH key (no passphrase for automation)
ssh-keygen -t rsa -b 4096 -f airflow_worker_key -N ""

# This creates:
# - airflow_worker_key (private key - stays on master)
# - airflow_worker_key.pub (public key - copy to workers)
            </div>
            <div class="warning-box">
                <strong>‚ö†Ô∏è Security:</strong> Keep private key secure! Only readable by airflow user.
                <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>chmod 600 /home/airflow/.ssh/airflow_worker_key</div>
            </div>
        </div>

        <!-- STEP 3 -->
        <div class="step">
            <h3><span class="step-number">3</span>Setup SSH Access to Remote Worker</h3>
            <strong>What this does:</strong> Copies public key to remote worker and tests connection
            <br><strong>Manual method:</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Copy public key to remote worker (enter worker password when prompted)
ssh-copy-id -i /home/airflow/.ssh/airflow_worker_key.pub airflow@45.151.155.74

# Test passwordless SSH connection
ssh -i /home/airflow/.ssh/airflow_worker_key airflow@45.151.155.74 "hostname"
# Should return remote hostname without password prompt
            </div>
            <strong>Or use helper script:</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Using setup_ssh_keys.sh script
./scripts/setup_ssh_keys.sh 45.151.155.74 airflow
            </div>
            <strong>What happens on remote worker:</strong>
            <ul>
                <li>Public key added to <code>~/.ssh/authorized_keys</code></li>
                <li>Permissions set correctly (700 for .ssh, 600 for authorized_keys)</li>
                <li>SSH daemon configuration validated</li>
            </ul>
        </div>

        <!-- STEP 4 -->
        <div class="step">
            <h3><span class="step-number">4</span>Deploy Worker Launcher Script to Remote</h3>
            <strong>What this does:</strong> Copies the worker startup script to remote worker
            <br><strong>How to do it:</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Copy launcher script to remote worker
scp -i /home/airflow/.ssh/airflow_worker_key \
    scripts/worker_launcher.sh \
    airflow@45.151.155.74:/home/airflow/

# Make it executable
ssh -i /home/airflow/.ssh/airflow_worker_key airflow@45.151.155.74 \
    "chmod +x /home/airflow/worker_launcher.sh"
            </div>
            <strong>What worker_launcher.sh does:</strong>
            <ul>
                <li>Establishes SSH reverse tunnels back to master (PostgreSQL, broker)</li>
                <li>Starts Celery worker in Singularity container</li>
                <li>Connects worker to master via tunnels</li>
                <li>Executes tasks assigned to this worker</li>
                <li>Stops when tasks complete (on-demand)</li>
            </ul>
        </div>

        <!-- STEP 5 -->
        <div class="step">
            <h3><span class="step-number">5</span>Test SSH Tunnel Connectivity</h3>
            <strong>What this tests:</strong> Verifies SSH tunnel can forward ports from worker to master
            <br><strong>How to test:</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# From master, create test tunnel to PostgreSQL
ssh -i /home/airflow/.ssh/airflow_worker_key \
    -L 15432:localhost:5432 \
    airflow@45.151.155.74 \
    -N -f

# Test PostgreSQL connection through tunnel
psql -h localhost -p 15432 -U airflow -d airflow_db -c "SELECT 1;"

# If successful, close tunnel
pkill -f "ssh.*15432:localhost:5432"
            </div>
            <div class="info-box">
                <strong>Tunnel Explanation:</strong>
                <ul>
                    <li><code>-L 15432:localhost:5432</code> - Forward local port 15432 to master's PostgreSQL port 5432</li>
                    <li><code>-N</code> - Don't execute remote command</li>
                    <li><code>-f</code> - Run in background</li>
                </ul>
            </div>
        </div>

        <!-- STEP 6 -->
        <div class="step">
            <h3><span class="step-number">6</span>Add Worker to Airflow Registry</h3>
            <strong>What this does:</strong> Registers new worker in Airflow Variables (NO code changes needed)
            <br><strong>Method A: Using Web UI</strong>
            <ol>
                <li>Login to Airflow Web UI</li>
                <li>Navigate to <strong>Admin ‚Üí Variables</strong></li>
                <li>Click <strong>"+"</strong> to add new variable or edit existing <code>WORKER_REGISTRY</code></li>
                <li>Set Key: <code>WORKER_REGISTRY</code></li>
                <li>Set Value (JSON):
                    <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
{
  "workers": {
    "worker-1": {
      "host": "45.151.155.74",
      "ssh_port": 22,
      "ssh_user": "airflow",
      "priority": 1,
      "status": "available",
      "container_type": "singularity"
    }
  }
}
                    </div>
                </li>
                <li>Click <strong>Save</strong></li>
            </ol>
            <strong>Method B: Using CLI Tool</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Add worker using manage_workers.py script
cd /home/airflow/airflow/scripts/
python manage_workers.py add worker-1 45.151.155.74 \
    --user airflow \
    --priority 1 \
    --container singularity

# List all workers
python manage_workers.py list

# Output:
# Worker ID   | Host           | Status    | Priority
# worker-1    | 45.151.155.74  | available | 1
            </div>
            <strong>Method C: Using Airflow CLI</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Set variable directly
airflow variables set WORKER_REGISTRY '{
  "workers": {
    "worker-1": {
      "host": "45.151.155.74",
      "ssh_port": 22,
      "ssh_user": "airflow",
      "priority": 1,
      "status": "available",
      "container_type": "singularity"
    }
  }
}'

# Verify
airflow variables get WORKER_REGISTRY
            </div>
        </div>

        <!-- STEP 7 -->
        <div class="step">
            <h3><span class="step-number">7</span>Deploy Worker Health Monitor DAG</h3>
            <strong>What this does:</strong> Automatically checks worker availability every 5 minutes
            <br><strong>How it works:</strong>
            <ul>
                <li>Reads all workers from <code>WORKER_REGISTRY</code></li>
                <li>Tests SSH connection to each worker</li>
                <li>Updates <code>status</code> field: "available" or "unavailable"</li>
                <li>Runs on schedule: <code>*/5 * * * *</code> (every 5 minutes)</li>
            </ul>
            <strong>To activate:</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# The DAG file already exists: dags/worker_health_monitor.py
# Enable it in Airflow UI
airflow dags unpause worker_health_monitor

# Check it's running
airflow dags list | grep worker_health_monitor
            </div>
        </div>

        <!-- STEP 8 -->
        <div class="step">
            <h3><span class="step-number">8</span>Test Worker with Example DAG</h3>
            <strong>What this does:</strong> Validates end-to-end worker execution
            <br><strong>How to test:</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Trigger example DAG
airflow dags trigger example_dag_with_workers

# Monitor execution in Airflow UI or CLI
airflow dags state example_dag_with_workers
            </div>
            <strong>What should happen:</strong>
            <ol>
                <li>DAG triggers with worker priority list: <code>['worker-1', 'worker-2']</code></li>
                <li><code>SmartRemoteWorkerOperator</code> reads registry and selects worker-1</li>
                <li>SSH connection established to 45.151.155.74</li>
                <li>SSH tunnels created (PostgreSQL: 5432, Broker: 6379)</li>
                <li>Worker launcher script executes on remote</li>
                <li>Celery worker starts in container</li>
                <li>Task executes on remote worker</li>
                <li>Result reported back through tunnel</li>
                <li>Worker stops after task completion</li>
                <li>SSH connection closes</li>
            </ol>
            <strong>Check logs:</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# View task logs in Airflow UI
# Or via CLI
airflow tasks logs example_dag_with_workers task_name 2024-10-25
            </div>
        </div>

        <!-- ===== HOW TO ADD MORE WORKERS ===== -->
        <h2>‚ûï How to Add New Remote Workers</h2>

        <div class="info-box">
            <strong>üéØ Goal:</strong> Add second worker (worker-2) at IP 10.0.1.20 with priority 2
        </div>

        <h3>Quick Steps</h3>
        <div class="step">
            <strong>1. Setup SSH Access</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Copy SSH key to new worker
ssh-copy-id -i /home/airflow/.ssh/airflow_worker_key.pub airflow@10.0.1.20

# Test connection
ssh -i /home/airflow/.ssh/airflow_worker_key airflow@10.0.1.20 "hostname"
            </div>
        </div>

        <div class="step">
            <strong>2. Deploy Worker Launcher</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Copy launcher script
scp -i /home/airflow/.ssh/airflow_worker_key \
    scripts/worker_launcher.sh \
    airflow@10.0.1.20:/home/airflow/

# Make executable
ssh -i /home/airflow/.ssh/airflow_worker_key airflow@10.0.1.20 \
    "chmod +x /home/airflow/worker_launcher.sh"
            </div>
        </div>

        <div class="step">
            <strong>3. Add to Registry</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Using CLI tool
python scripts/manage_workers.py add worker-2 10.0.1.20 \
    --user airflow \
    --priority 2 \
    --container singularity

# Verify
python scripts/manage_workers.py list

# Output:
# Worker ID   | Host           | Status    | Priority
# worker-1    | 45.151.155.74  | available | 1
# worker-2    | 10.0.1.20      | available | 2
            </div>
        </div>

        <div class="step">
            <strong>4. Test New Worker</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Trigger DAG with new worker in priority list
airflow dags trigger example_dag_with_workers \
    --conf '{"worker_priority": ["worker-2", "worker-1"]}'
            </div>
        </div>

        <div class="warning-box">
            <strong>‚ö†Ô∏è Important:</strong> No code changes needed! Just:
            <ul>
                <li>Setup SSH access</li>
                <li>Deploy launcher script</li>
                <li>Add to registry via UI/CLI</li>
                <li>Done!</li>
            </ul>
        </div>

        <!-- ===== HOW IT WORKS ===== -->
        <h2>üîÑ How Priority Selection & Retry Works</h2>

        <h3>Scenario 1: Normal Execution</h3>
        <div class="flow-diagram">
            <div style="background: #fff9c4; padding: 10px; margin: 5px;">
                DAG triggers with priority: ['worker-1', 'worker-2']
            </div>
            <div class="arrow">‚Üì</div>
            <div style="background: #e3f2fd; padding: 10px; margin: 5px;">
                SmartRemoteWorkerOperator reads WORKER_REGISTRY
            </div>
            <div class="arrow">‚Üì</div>
            <div style="background: #e3f2fd; padding: 10px; margin: 5px;">
                Check worker-1 status: "available" ‚úì
            </div>
            <div class="arrow">‚Üì</div>
            <div style="background: #c8e6c9; padding: 10px; margin: 5px;">
                Select worker-1 (45.151.155.74)
            </div>
            <div class="arrow">‚Üì</div>
            <div style="background: #c8e6c9; padding: 10px; margin: 5px;">
                Execute task successfully
            </div>
        </div>

        <h3>Scenario 2: Worker Unavailable (5-Minute Retry)</h3>
        <div class="flow-diagram">
            <div style="background: #fff9c4; padding: 10px; margin: 5px;">
                DAG triggers with priority: ['worker-1', 'worker-2']
            </div>
            <div class="arrow">‚Üì</div>
            <div style="background: #ffcdd2; padding: 10px; margin: 5px;">
                Check worker-1 status: "unavailable" ‚úó
            </div>
            <div class="arrow">‚Üì</div>
            <div style="background: #fff3cd; padding: 10px; margin: 5px;">
                <strong>Wait 5 minutes</strong> (retry logic)
            </div>
            <div class="arrow">‚Üì</div>
            <div style="background: #ffcdd2; padding: 10px; margin: 5px;">
                Check worker-1 again: Still "unavailable" ‚úó
            </div>
            <div class="arrow">‚Üì</div>
            <div style="background: #e3f2fd; padding: 10px; margin: 5px;">
                Move to next in priority: worker-2
            </div>
            <div class="arrow">‚Üì</div>
            <div style="background: #c8e6c9; padding: 10px; margin: 5px;">
                Check worker-2 status: "available" ‚úì
            </div>
            <div class="arrow">‚Üì</div>
            <div style="background: #c8e6c9; padding: 10px; margin: 5px;">
                Execute task on worker-2 successfully
            </div>
        </div>

        <h3>Scenario 3: Worker Fails During Execution</h3>
        <div class="flow-diagram">
            <div style="background: #fff9c4; padding: 10px; margin: 5px;">
                Task executing on worker-1
            </div>
            <div class="arrow">‚Üì</div>
            <div style="background: #ffcdd2; padding: 10px; margin: 5px;">
                <strong>Worker-1 connection lost</strong> (network issue / crash)
            </div>
            <div class="arrow">‚Üì</div>
            <div style="background: #fff3cd; padding: 10px; margin: 5px;">
                <strong>Wait 5 minutes</strong> (retry to reconnect)
            </div>
            <div class="arrow">‚Üì</div>
            <div style="background: #ffcdd2; padding: 10px; margin: 5px;">
                Still cannot reach worker-1 ‚úó
            </div>
            <div class="arrow">‚Üì</div>
            <div style="background: #fff3cd; padding: 10px; margin: 5px;">
                <strong>Kill task on main node</strong>
            </div>
            <div class="arrow">‚Üì</div>
            <div style="background: #e3f2fd; padding: 10px; margin: 5px;">
                Select next worker: worker-2
            </div>
            <div class="arrow">‚Üì</div>
            <div style="background: #c8e6c9; padding: 10px; margin: 5px;">
                <strong>Re-run DAG on worker-2</strong>
            </div>
        </div>

        <!-- ===== USAGE IN DAGS ===== -->
        <h2>üìù How to Use in Your DAGs</h2>

        <h3>Example DAG Structure (No Code, Just Explanation)</h3>
        
        <div class="step">
            <strong>File:</strong> <code>dags/my_pipeline.py</code>
            <br><strong>What it contains:</strong>
            <ul>
                <li>Import <code>SmartRemoteWorkerOperator</code> from plugins</li>
                <li>Define DAG with default parameters</li>
                <li>Specify worker priority list in DAG params or task params</li>
                <li>Create tasks using the operator</li>
                <li>Tasks automatically use priority selection and retry logic</li>
            </ul>
            <strong>Priority specification options:</strong>
            <ol>
                <li><strong>Explicit list:</strong> <code>['worker-1', 'worker-2']</code> - Try in this exact order</li>
                <li><strong>Auto-select:</strong> <code>'auto'</code> - Use global priority from registry</li>
                <li><strong>Tag-based:</strong> <code>['tag:gpu', 'tag:high-memory']</code> - Select by worker tags</li>
                <li><strong>All available:</strong> <code>None</code> - Use any available worker</li>
            </ol>
        </div>

        <h3>How SmartRemoteWorkerOperator Works</h3>
        <div class="info-box">
            <strong>Execution Flow Inside Operator:</strong>
            <ol>
                <li><strong>Read Registry:</strong> Get <code>WORKER_REGISTRY</code> from Airflow Variables</li>
                <li><strong>Filter Workers:</strong> Apply priority list from DAG params</li>
                <li><strong>Check Availability:</strong> Verify status field for each worker in priority order</li>
                <li><strong>Select Worker:</strong> Choose first available worker</li>
                <li><strong>Establish SSH Connection:</strong> Connect using credentials from registry</li>
                <li><strong>Create Tunnels:</strong> Forward PostgreSQL (5432) and Broker (6379) ports</li>
                <li><strong>Start Worker:</strong> Execute <code>worker_launcher.sh</code> on remote</li>
                <li><strong>Monitor Task:</strong> Wait for task completion</li>
                <li><strong>Handle Failures:</strong> If worker fails, wait 5 min, retry once, then try next worker</li>
                <li><strong>Cleanup:</strong> Stop worker and close SSH connections</li>
            </ol>
        </div>

        <!-- ===== MANAGEMENT OPERATIONS ===== -->
        <h2>üõ†Ô∏è Common Management Operations</h2>

        <h3>List All Workers</h3>
        <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Using CLI tool
python scripts/manage_workers.py list

# Or via Airflow UI
Admin ‚Üí Variables ‚Üí Click on WORKER_REGISTRY ‚Üí View JSON
        </div>

        <h3>Update Worker Status Manually</h3>
        <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Mark worker as unavailable (for maintenance)
python scripts/manage_workers.py set-status worker-1 unavailable

# Mark back as available
python scripts/manage_workers.py set-status worker-1 available
        </div>

        <h3>Remove Worker</h3>
        <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Remove worker from registry
python scripts/manage_workers.py remove worker-1

# Verify removal
python scripts/manage_workers.py list
        </div>

        <h3>Update Worker Priority</h3>
        <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Change priority (lower number = higher priority)
python scripts/manage_workers.py set-priority worker-2 1

# This makes worker-2 the first choice
        </div>

        <h3>View Worker Health Status</h3>
        <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Check last health check time
airflow dags state worker_health_monitor

# View health monitor logs
airflow tasks logs worker_health_monitor check_workers_task 2024-10-25
        </div>

        <!-- ===== SMART REMOTE WORKER V2 ===== -->
        <h2>üéØ Smart Remote Worker V2 - The Final Solution (RECOMMENDED)</h2>

        <div class="feature-box">
            <h3>‚úÖ WORKING SOLUTION - Use This!</h3>
            <p><strong>Smart Remote Worker Operator V2</strong> combines the best of both approaches:</p>
            <ul>
                <li>‚úÖ <strong>Dynamic Worker Selection</strong> - Reads from WORKER_REGISTRY, no hardcoded IPs</li>
                <li>‚úÖ <strong>Reliable Execution</strong> - Uses Airflow's SSHHook (proven and stable)</li>
                <li>‚úÖ <strong>Auto SSH Connections</strong> - Creates connections automatically</li>
                <li>‚úÖ <strong>Priority-Based Routing</strong> - Try workers in order you specify</li>
                <li>‚úÖ <strong>Automatic Failover</strong> - If one worker fails, tries next</li>
                <li>‚úÖ <strong>Singularity Support</strong> - Auto-detects and uses your containers</li>
                <li>‚úÖ <strong>Simple & Stable</strong> - No complex worker lifecycle management</li>
            </ul>
        </div>

        <h3>üìã Quick Start (3 Steps)</h3>

        <div class="step">
            <h4><span class="step-number">1</span>Update Worker Configuration</h4>
            <p>Set the correct AIRFLOW_HOME for your remote worker:</p>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# On master server
cd ~/airflow

# Update worker registry with remote worker's AIRFLOW_HOME
python update_worker_config.py
            </div>
            
            <p><strong>What this does:</strong> Adds <code>airflow_home: /home/airflow/airflow-worker</code> to worker-1 configuration</p>
            
            <div class="info-box">
                <strong>Note:</strong> Master and remote can have different AIRFLOW_HOME paths! This is normal and correct.
                <ul>
                    <li>Master: <code>/home/airflow/airflow</code> (manages orchestration)</li>
                    <li>Worker: <code>/home/airflow/airflow-worker</code> (executes tasks)</li>
                </ul>
            </div>
        </div>

        <div class="step">
            <h4><span class="step-number">2</span>Verify Configuration</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Check worker configuration
python scripts/manage_workers.py show worker-1
            </div>
            
            <p><strong>Expected output:</strong></p>
            <div class="command">
============================================================
üìã WORKER DETAILS: worker-1
============================================================

Connection:
  Host:          45.151.155.74
  User:          airflow
  Port:          22

Configuration:
  Status:        ‚úÖ available
  Priority:      1
  Container:     singularity
  AIRFLOW_HOME:  /home/airflow/airflow-worker  ‚Üê Should see this!

============================================================
            </div>
        </div>

        <div class="step">
            <h4><span class="step-number">3</span>Test the Operator</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Trigger test DAG
airflow dags trigger test_smart_v2

# Wait 30 seconds
sleep 30

# Check logs
cat ~/airflow/logs/dag_id=test_smart_v2/run_id=manual*/task_id=test_simple_command/attempt=1.log | tail -80
            </div>
            
            <p><strong>Expected success output:</strong></p>
            <div class="command">
[INFO] üöÄ Smart Remote Worker Operator V2 - Starting
[INFO] üìã Loaded 1 workers from registry
[INFO] üìã Selected 1 worker(s)
[INFO]    - worker-1 (priority: 1, host: 45.151.155.74)
[INFO] üéØ ATTEMPT 1/1: Trying worker 'worker-1'
[INFO] üéØ Executing on worker: worker-1
[INFO]    Host: 45.151.155.74
[INFO]    AIRFLOW_HOME: /home/airflow/airflow-worker
[INFO] ‚úÖ Created SSH connection: worker_worker-1
[INFO] üì§ Executing command via SSH...
[INFO] üìä Exit status: 0
[INFO] üìÑ Output:
==========================================
Simple test on remote worker
Hostname: remotedimpal-2
Date: Sat Oct 25 05:30:00 UTC 2025
PWD: /home/airflow/airflow-worker
User: airflow
==========================================
[INFO] ‚úÖ Task completed successfully on worker-1
[INFO] ‚úÖ SUCCESS: Task completed
            </div>
        </div>

        <h3>üéØ Using V2 Operator in Your DAGs</h3>

        <div class="step">
            <h4>Example 1: Simple Task</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
from datetime import datetime
from airflow import DAG
import sys, os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'plugins'))
from operators.smart_remote_worker_v2 import SmartRemoteWorkerOperatorV2

with DAG(
    'my_dag',
    start_date=datetime(2024, 1, 1),
    schedule=None,
    catchup=False,
) as dag:
    
    task = SmartRemoteWorkerOperatorV2(
        task_id='my_remote_task',
        bash_command='echo "Hello from remote worker!"',
        worker_priority=['worker-1'],  # Try worker-1
        dag=dag,
    )
            </div>
        </div>

        <div class="step">
            <h4>Example 2: Auto-Select Worker</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
task = SmartRemoteWorkerOperatorV2(
    task_id='auto_select_task',
    bash_command='''
    echo "Processing data..."
    python /path/to/script.py
    echo "Done!"
    ''',
    worker_priority=[],  # Empty list = auto-select by priority
    dag=dag,
)
            </div>
            <p>When <code>worker_priority=[]</code>, operator uses all available workers sorted by priority number (1 = highest)</p>
        </div>

        <div class="step">
            <h4>Example 3: Failover with Multiple Workers</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
task = SmartRemoteWorkerOperatorV2(
    task_id='failover_task',
    bash_command='./process_large_dataset.sh',
    worker_priority=['gpu-worker-1', 'gpu-worker-2', 'worker-1'],
    retry_delay_seconds=120,  # Wait 2 minutes before retry
    dag=dag,
)
            </div>
            <p>If gpu-worker-1 fails, tries gpu-worker-2, then worker-1. After all fail, waits 2 minutes and retries.</p>
        </div>

        <div class="step">
            <h4>Example 4: Complex Data Processing</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
process_data = SmartRemoteWorkerOperatorV2(
    task_id='process_data',
    bash_command='''
    cd $AIRFLOW_HOME/data
    
    echo "Starting data processing at $(date)"
    echo "Worker: $(hostname)"
    echo "Directory: $(pwd)"
    
    # Process data
    python process_dataset.py --input raw/ --output processed/
    
    # Verify output
    ls -lh processed/
    
    echo "Processing complete at $(date)"
    ''',
    worker_priority=['worker-1'],
    dag=dag,
)
            </div>
        </div>

        <h3>üìä Comparison: V1 vs V2</h3>

        <table>
            <tr>
                <th>Feature</th>
                <th>V1 (Old)</th>
                <th>V2 (New) ‚úÖ</th>
            </tr>
            <tr>
                <td>Dynamic Worker Selection</td>
                <td>‚úÖ Yes</td>
                <td>‚úÖ Yes</td>
            </tr>
            <tr>
                <td>SSH Connection</td>
                <td>‚ùå Raw paramiko (unstable)</td>
                <td>‚úÖ SSHHook (reliable)</td>
            </tr>
            <tr>
                <td>Auto-creates Connections</td>
                <td>‚ùå No</td>
                <td>‚úÖ Yes</td>
            </tr>
            <tr>
                <td>Worker Lifecycle Management</td>
                <td>‚ùå Complex (caused issues)</td>
                <td>‚úÖ Simple (just execute)</td>
            </tr>
            <tr>
                <td>Singularity Support</td>
                <td>‚úÖ Yes</td>
                <td>‚úÖ Yes (auto-detect)</td>
            </tr>
            <tr>
                <td>Priority-based Routing</td>
                <td>‚úÖ Yes</td>
                <td>‚úÖ Yes</td>
            </tr>
            <tr>
                <td>Automatic Failover</td>
                <td>‚úÖ Yes</td>
                <td>‚úÖ Yes</td>
            </tr>
            <tr>
                <td>Stability</td>
                <td>‚ùå Had pkill/SSH issues</td>
                <td>‚úÖ Stable</td>
            </tr>
            <tr>
                <td>Code Complexity</td>
                <td>‚ùå Complex (619 lines)</td>
                <td>‚úÖ Simpler (320 lines)</td>
            </tr>
        </table>

        <h3>üîß Configuration Options</h3>

        <table>
            <tr>
                <th>Parameter</th>
                <th>Type</th>
                <th>Default</th>
                <th>Description</th>
            </tr>
            <tr>
                <td><code>bash_command</code></td>
                <td>str</td>
                <td>(required)</td>
                <td>Command to execute on remote worker</td>
            </tr>
            <tr>
                <td><code>worker_priority</code></td>
                <td>list</td>
                <td>[]</td>
                <td>List of worker names to try in order. Empty = auto-select all by priority</td>
            </tr>
            <tr>
                <td><code>retry_delay_seconds</code></td>
                <td>int</td>
                <td>300</td>
                <td>Seconds to wait before retry if all workers fail</td>
            </tr>
            <tr>
                <td><code>queue</code></td>
                <td>str</td>
                <td>'default'</td>
                <td>Queue name (for future use)</td>
            </tr>
        </table>

        <h3>üóëÔ∏è Cleanup Old Files</h3>

        <div class="warning-box">
            <h4>‚ö†Ô∏è Remove Obsolete Files</h4>
            <p>Now that V2 is working, you can safely remove old/buggy files:</p>
        </div>

        <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Run cleanup script (creates backup first)
chmod +x cleanup_old_files.sh
./cleanup_old_files.sh
        </div>

        <p><strong>Files that will be removed:</strong></p>
        <ul>
            <li><code>plugins/operators/smart_remote_worker.py</code> - Old buggy version</li>
            <li><code>dags/example_dag_with_workers.py</code> - Old example using V1</li>
            <li>Various debug/fix markdown files (info moved to this HTML)</li>
        </ul>

        <p><strong>Files that are KEPT:</strong></p>
        <ul>
            <li>‚úÖ <code>plugins/operators/smart_remote_worker_v2.py</code> - Working V2 operator</li>
            <li>‚úÖ <code>plugins/operators/remote_worker_operator_fixed.py</code> - Reference</li>
            <li>‚úÖ <code>dags/test_smart_v2.py</code> - Working test DAG</li>
            <li>‚úÖ <code>scripts/manage_workers.py</code> - Worker management CLI</li>
            <li>‚úÖ <code>update_worker_config.py</code> - Configuration helper</li>
            <li>‚úÖ <code>SMART_WORKER_SETUP_GUIDE.html</code> - This guide</li>
        </ul>

        <h3>üìù Worker Management Commands</h3>

        <div class="step">
            <h4>View All Workers</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
python scripts/manage_workers.py list
            </div>
        </div>

        <div class="step">
            <h4>Add New Worker</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Add worker-2
python scripts/manage_workers.py add worker-2 IP_ADDRESS --user airflow --priority 2

# Update its airflow_home
python -c "
from airflow.models import Variable
import json
registry = json.loads(Variable.get('WORKER_REGISTRY'))
registry['workers']['worker-2']['airflow_home'] = '/home/airflow/airflow-worker'
Variable.set('WORKER_REGISTRY', json.dumps(registry))
print('‚úÖ Updated worker-2 airflow_home')
"
            </div>
        </div>

        <div class="step">
            <h4>Update Worker Status</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Mark worker unavailable for maintenance
python scripts/manage_workers.py set-status worker-1 maintenance

# Mark back as available
python scripts/manage_workers.py set-status worker-1 available
            </div>
        </div>

        <div class="step">
            <h4>Remove Worker</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
python scripts/manage_workers.py remove worker-2
            </div>
        </div>

        <h3>‚úÖ Testing Checklist</h3>

        <div class="step">
            <strong>1. Verify SSH Connection</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
ssh airflow@45.151.155.74 "hostname && echo 'SSH OK'"
            </div>
        </div>

        <div class="step">
            <strong>2. Check Worker Registry</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
python scripts/manage_workers.py show worker-1 | grep "AIRFLOW_HOME"
# Should show: AIRFLOW_HOME:  /home/airflow/airflow-worker
            </div>
        </div>

        <div class="step">
            <strong>3. Verify Singularity Image</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
ssh airflow@45.151.155.74 "ls -lh /home/airflow/airflow-worker/containers/*.sif"
# Should show: airflow-worker.sif
            </div>
        </div>

        <div class="step">
            <strong>4. Test V2 Operator</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
airflow dags trigger test_smart_v2
sleep 30
airflow dags list-runs -d test_smart_v2 -o table --num-runs 1
            </div>
        </div>

        <div class="step">
            <strong>5. Check Task Logs</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Look for these success indicators:
cat ~/airflow/logs/dag_id=test_smart_v2/run_id=manual*/task_id=test_simple_command/attempt=1.log | grep -E "‚úÖ|Exit status: 0|SUCCESS"
            </div>
        </div>

        <!-- ===== INTEGRATION TESTING ===== -->
        <h2>üß™ Full Integration Testing</h2>

        <div class="feature-box">
            <h3>Complete End-to-End Test Suite</h3>
            <p>Automated testing to validate your entire setup:</p>
            <ul>
                <li>‚úÖ Start ‚Üí Connect to Remote ‚Üí Execute Tasks ‚Üí PostgreSQL/Redis ‚Üí Complete</li>
                <li>‚úÖ Tests all components of the system</li>
                <li>‚úÖ Automated monitoring and reporting</li>
                <li>‚úÖ Quick validation (30 seconds) or Full test (5 minutes)</li>
            </ul>
        </div>

        <h3>üì¶ Test Suite Components</h3>

        <table>
            <tr>
                <th>File</th>
                <th>Purpose</th>
                <th>Time</th>
            </tr>
            <tr>
                <td><code>quick_test.sh</code></td>
                <td>Fast validation of basic setup</td>
                <td>30 seconds</td>
            </tr>
            <tr>
                <td><code>run_full_integration_test.sh</code></td>
                <td>Complete automated test runner</td>
                <td>5 minutes</td>
            </tr>
            <tr>
                <td><code>dags/full_integration_test.py</code></td>
                <td>Integration test DAG (13 tasks)</td>
                <td>Triggered by runner</td>
            </tr>
        </table>

        <h3>üöÄ Quick Test (Recommended First)</h3>

        <div class="step">
            <h4>Run 30-Second Validation</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
cd ~/airflow
chmod +x quick_test.sh
./quick_test.sh
            </div>

            <p><strong>What it tests:</strong></p>
            <ul>
                <li>SSH connection to remote worker</li>
                <li>Remote AIRFLOW_HOME</li>
                <li>Directory structure</li>
                <li>Singularity images</li>
                <li>Worker registry</li>
                <li>Remote command execution</li>
                <li>Network connectivity</li>
            </ul>

            <p><strong>Expected output:</strong></p>
            <div class="command">
========================================
üöÄ QUICK VALIDATION TEST
========================================

‚ÑπÔ∏è  Test 1/7: SSH Connection to Remote Worker...
‚úÖ SSH connection works (remote: remotedimpal-2)

‚ÑπÔ∏è  Test 2/7: Remote AIRFLOW_HOME...
‚úÖ AIRFLOW_HOME: /home/airflow/airflow-worker

‚ÑπÔ∏è  Test 3/7: Remote Directory Structure...
‚úÖ Remote Airflow directory exists

‚ÑπÔ∏è  Test 4/7: Singularity Image...
‚úÖ Singularity image found: airflow-worker.sif

‚ÑπÔ∏è  Test 5/7: Worker Registry...
‚úÖ Worker registry found (1 workers)

‚ÑπÔ∏è  Test 6/7: Remote Command Execution...
‚úÖ Remote command execution works

‚ÑπÔ∏è  Test 7/7: PostgreSQL Connectivity...
‚úÖ Remote can reach PostgreSQL on master

========================================
‚úÖ QUICK TEST COMPLETE
========================================

All quick tests passed! üéâ
            </div>
        </div>

        <h3>üî¨ Full Integration Test</h3>

        <div class="step">
            <h4>Run Complete Test Suite</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
cd ~/airflow
chmod +x run_full_integration_test.sh
./run_full_integration_test.sh
            </div>

            <p><strong>Test Flow:</strong></p>
            <div style="background: #f5f5f5; padding: 15px; border-radius: 5px; font-family: monospace;">
                Start<br>
                &nbsp;&nbsp;‚Üì<br>
                Pre-flight Checks (SSH, workers, Airflow)<br>
                &nbsp;&nbsp;‚Üì<br>
                Trigger Integration Test DAG<br>
                &nbsp;&nbsp;‚Üì<br>
                Monitor Execution (real-time)<br>
                &nbsp;&nbsp;‚Üì<br>
                Test SSH Connection ‚úÖ<br>
                &nbsp;&nbsp;‚Üì<br>
                Test Basic Execution ‚úÖ<br>
                &nbsp;&nbsp;‚Üì<br>
                Test PostgreSQL ‚îÄ‚îÄ Test Redis<br>
                &nbsp;&nbsp;‚Üì<br>
                Test Complex Task ‚úÖ<br>
                &nbsp;&nbsp;‚Üì<br>
                Test Singularity ‚úÖ<br>
                &nbsp;&nbsp;‚Üì<br>
                Test Error Handling ‚úÖ<br>
                &nbsp;&nbsp;‚Üì<br>
                Collect Results<br>
                &nbsp;&nbsp;‚Üì<br>
                Cleanup<br>
                &nbsp;&nbsp;‚Üì<br>
                Generate Report<br>
                &nbsp;&nbsp;‚Üì<br>
                End
            </div>
        </div>

        <div class="step">
            <h4>Expected Results</h4>
            <div class="command">
========================================
TEST REPORT
========================================

Date: Sat Oct 25 06:20:15 UTC 2025
DAG: full_integration_test
Run ID: manual__2025-10-25T06:15:00+00:00

‚úÖ Overall Status: PASSED ‚úÖ

Summary:
  ‚úÖ SSH connection to remote worker: OK
  ‚úÖ Remote command execution: OK
  ‚úÖ PostgreSQL access test: Completed
  ‚úÖ Redis access test: Completed
  ‚úÖ Complex task execution: OK
  ‚úÖ Singularity container test: Completed
  ‚úÖ Error handling test: OK

Detailed Results:
‚úÖ PASSED      - start
‚úÖ PASSED      - print_banner
‚úÖ PASSED      - get_connection_info
‚úÖ PASSED      - test_ssh_connection
‚úÖ PASSED      - test_basic_execution
‚úÖ PASSED      - test_postgresql_access
‚úÖ PASSED      - test_redis_access
‚úÖ PASSED      - test_complex_task
‚úÖ PASSED      - test_singularity_execution
‚úÖ PASSED      - test_error_handling
‚úÖ PASSED      - collect_results
‚úÖ PASSED      - cleanup
‚úÖ PASSED      - end

========================================
üéâ INTEGRATION TEST PASSED!
========================================
            </div>
        </div>

        <h3>üìã Test Commands Reference</h3>

        <div class="step">
            <h4>Quick Test</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Run quick validation (30 seconds)
./quick_test.sh
            </div>
        </div>

        <div class="step">
            <h4>Full Test</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Run full integration test (5 minutes)
./run_full_integration_test.sh

# View recent test status
./run_full_integration_test.sh status

# View specific task logs
./run_full_integration_test.sh logs test_ssh_connection

# Clean old test runs
./run_full_integration_test.sh clean
            </div>
        </div>

        <div class="step">
            <h4>Manual DAG Trigger</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Trigger test via CLI
airflow dags trigger full_integration_test

# Check status
airflow dags list-runs -d full_integration_test -o table --num-runs 1

# View in Web UI
# Go to: http://localhost:8080/dags/full_integration_test/grid
            </div>
        </div>

        <h3>‚ö†Ô∏è Optional Tests (May Show Warnings)</h3>

        <div class="info-box">
            <p><strong>PostgreSQL and Redis Tests:</strong></p>
            <p>These tests try to connect directly from the remote worker to master's PostgreSQL and Redis.</p>
            <p><strong>‚ö†Ô∏è These may fail with warnings, and that's NORMAL!</strong></p>
            <ul>
                <li>V2 operator <strong>does NOT require</strong> direct database access from workers</li>
                <li>Workers only need SSH access to master</li>
                <li>If these tests show warnings, your system still works perfectly!</li>
            </ul>
            <p><strong>When you see:</strong> "Cannot connect to PostgreSQL" or "Cannot connect to Redis"</p>
            <p><strong>This means:</strong> Database is not exposed to remote workers (which is fine and more secure!)</p>
        </div>

        <h3>üéØ What Tests Validate</h3>

        <table>
            <tr>
                <th>Component</th>
                <th>Test</th>
                <th>Required?</th>
            </tr>
            <tr>
                <td>SSH Connection</td>
                <td>Connect to remote worker</td>
                <td>‚úÖ Yes</td>
            </tr>
            <tr>
                <td>Remote Execution</td>
                <td>Run commands on remote</td>
                <td>‚úÖ Yes</td>
            </tr>
            <tr>
                <td>Worker Registry</td>
                <td>Configuration is valid</td>
                <td>‚úÖ Yes</td>
            </tr>
            <tr>
                <td>V2 Operator</td>
                <td>All operator features work</td>
                <td>‚úÖ Yes</td>
            </tr>
            <tr>
                <td>PostgreSQL Direct Access</td>
                <td>Remote can reach master DB</td>
                <td>‚ùå Optional</td>
            </tr>
            <tr>
                <td>Redis Direct Access</td>
                <td>Remote can reach master Redis</td>
                <td>‚ùå Optional</td>
            </tr>
            <tr>
                <td>Singularity</td>
                <td>Container execution</td>
                <td>‚ùå Optional</td>
            </tr>
        </table>

        <h3>üîó SSH Tunnel Support (NEW!)</h3>

        <div class="feature-box">
            <h4>‚úÖ SSH Tunnels Now Implemented!</h4>
            <p>The V2 operator now automatically creates SSH tunnels from remote workers to master for PostgreSQL and Redis access.</p>
            <p><strong>Features:</strong></p>
            <ul>
                <li>‚úÖ Automatic tunnel creation before task execution</li>
                <li>‚úÖ PostgreSQL tunnel (port 5432)</li>
                <li>‚úÖ Redis/Celery broker tunnel (port 6379)</li>
                <li>‚úÖ Automatic tunnel cleanup after task completion</li>
                <li>‚úÖ Works even if tunnels fail (graceful degradation)</li>
            </ul>
        </div>

        <h4>Configuration</h4>

        <div class="step">
            <h4>Step 1: Set Master Node IP (Required)</h4>
            <p>Workers need to know the master node IP to create tunnels back:</p>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Set master node IP in Airflow Variables
airflow variables set MASTER_NODE_IP "45.151.155.100"

# Or via Web UI:
# Go to Admin ‚Üí Variables ‚Üí Add
# Key: MASTER_NODE_IP
# Value: 45.151.155.100
            </div>
        </div>

        <div class="step">
            <h4>Step 2: Verify SSH Keys (Required)</h4>
            <p>Workers must be able to SSH back to master node:</p>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# On remote worker, test SSH to master
ssh airflow@45.151.155.100 "echo 'Can reach master'"

# If fails, set up SSH keys FROM worker TO master
ssh-keygen -t ed25519 -f ~/.ssh/id_ed25519 -N ""
ssh-copy-id airflow@45.151.155.100
            </div>
        </div>

        <div class="step">
            <h4>Step 3: Allow SSH Port Forwarding (Required)</h4>
            <p>On master node, ensure SSH allows port forwarding:</p>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# On master server, edit SSH config
sudo nano /etc/ssh/sshd_config

# Ensure these settings:
AllowTcpForwarding yes
GatewayPorts no
PermitTunnel yes

# Restart SSH
sudo systemctl restart sshd
            </div>
        </div>

        <div class="info-box">
            <h4>How SSH Tunnels Work:</h4>
            <p><strong>Flow:</strong></p>
            <ol>
                <li>Master connects to Worker via SSH</li>
                <li>Worker creates reverse SSH tunnel back to Master</li>
                <li>Worker's localhost:5432 ‚Üí Master's PostgreSQL</li>
                <li>Worker's localhost:6379 ‚Üí Master's Redis</li>
                <li>Task executes with DB/Redis access</li>
                <li>Tunnels automatically cleaned up</li>
            </ol>
        </div>

        <div class="step">
            <h4>Verify Tunnels Work</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Trigger a test task
airflow dags trigger test_smart_v2

# Check logs for tunnel creation
cat ~/airflow/logs/dag_id=test_smart_v2/run_id=manual*/task_id=*/attempt=1.log | grep -A5 "Creating SSH tunnels"

# Should see:
# üîó Creating SSH tunnels from worker to master...
# ‚úÖ PostgreSQL tunnel established on port 5432
# ‚úÖ Redis tunnel established on port 6379
            </div>
        </div>

        <h3>‚úÖ Success Criteria</h3>

        <div class="feature-box">
            <h4>Your system is production-ready if:</h4>
            <ul>
                <li>‚úÖ Quick test passes completely</li>
                <li>‚úÖ Full integration test shows all tasks as PASSED</li>
                <li>‚úÖ SSH connection works reliably</li>
                <li>‚úÖ Remote commands execute successfully</li>
                <li>‚úÖ V2 operator creates connections automatically</li>
            </ul>
            
            <h4>Optional warnings are OK:</h4>
            <ul>
                <li>‚ö†Ô∏è "Cannot connect to PostgreSQL from remote" - This is fine!</li>
                <li>‚ö†Ô∏è "Cannot connect to Redis from remote" - This is fine!</li>
                <li>‚ö†Ô∏è "Singularity not available" - Tasks run directly (fine!)</li>
            </ul>
        </div>

        <h3>üìñ Detailed Test Documentation</h3>

        <div class="info-box">
            <p>For complete testing documentation, see: <code>INTEGRATION_TEST_GUIDE.md</code></p>
            <p>Includes:</p>
            <ul>
                <li>Detailed explanation of each test</li>
                <li>Troubleshooting for test failures</li>
                <li>How to interpret results</li>
                <li>Command reference</li>
                <li>CI/CD integration examples</li>
            </ul>
        </div>

        <!-- ===== TROUBLESHOOTING ===== -->
        <h2>üîç Troubleshooting Guide</h2>

        <div class="info-box">
            <strong>Most Common Issues:</strong> Worker unavailable despite SSH working, health monitor not updating, SSH key problems. All solutions tested and verified!
        </div>

        <!-- Issue 0: Most Common - Worker Unavailable But SSH Works -->
        <h3>‚ùå Issue 0: Worker Shows Unavailable (But SSH Works) - MOST COMMON!</h3>

        <div class="step" style="background: #ffebee; border-left: 4px solid #f44336;">
            <h4>Symptoms:</h4>
            <div class="command">
python scripts/manage_workers.py list

Worker ID            Host                 Status       Priority
worker-1             45.151.155.74        ‚ùå unavailable 1
            </div>
            
            <h4>FIRST: Test SSH Connection</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Test if SSH works without password
ssh airflow@45.151.155.74 "hostname"
            </div>

            <div class="warning-box">
                <strong>If SSH connects without password:</strong> Keys are set up, just update status manually!<br>
                <strong>If SSH asks for password:</strong> Keys are NOT set up, follow Scenario B below.
            </div>

            <h4>Scenario A: SSH Works ‚úÖ (Quick Fix!)</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Simply set status to available
python scripts/manage_workers.py set-status worker-1 available

# Verify
python scripts/manage_workers.py list

# Should now show: worker-1  ‚úÖ available
            </div>

            <h4>Scenario B: SSH Asks for Password ‚ùå (Setup Keys)</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# 1. Generate SSH key
ssh-keygen -t ed25519 -f ~/.ssh/id_ed25519 -N ""

# 2. Copy to worker (enter password once)
ssh-copy-id airflow@45.151.155.74

# 3. Test passwordless login
ssh airflow@45.151.155.74 "hostname"

# 4. Set status
python scripts/manage_workers.py set-status worker-1 available

# 5. Verify
python scripts/manage_workers.py list
            </div>
        </div>

        <!-- Issue 0.5: Health Monitor -->
        <h3>üè• Issue 0.5: Health Monitor Not Updating Status</h3>

        <div class="step" style="background: #fff3cd; border-left: 4px solid #ffc107;">
            <h4>Symptoms:</h4>
            <ul>
                <li>Status never changes even after SSH is working</li>
                <li>Health monitor DAG runs but no updates</li>
            </ul>

            <h4>Solution: Check & Fix Health Monitor</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# 1. Check if DAG is paused
airflow dags list | grep worker_health_monitor

# 2. Unpause it
airflow dags unpause worker_health_monitor

# 3. Trigger it manually
airflow dags trigger worker_health_monitor

# 4. Wait and check
sleep 30
python scripts/manage_workers.py list

# 5. Check logs if still not working
airflow dags list-runs -d worker_health_monitor -o table
airflow tasks logs worker_health_monitor check_worker_health [DATE_FROM_ABOVE]
            </div>
        </div>

        <!-- Quick Fix Cheat Sheet -->
        <h3>‚ö° Quick Fix Cheat Sheet</h3>

        <div class="step" style="background: #e8f5e9; border-left: 4px solid #4caf50;">
            <table>
                <tr>
                    <th>Problem</th>
                    <th>Quick Solution</th>
                </tr>
                <tr>
                    <td>Worker unavailable but SSH works</td>
                    <td><code>python scripts/manage_workers.py set-status worker-1 available</code></td>
                </tr>
                <tr>
                    <td>SSH asks for password</td>
                    <td><code>ssh-copy-id airflow@45.151.155.74</code></td>
                </tr>
                <tr>
                    <td>Health monitor not running</td>
                    <td><code>airflow dags unpause worker_health_monitor && airflow dags trigger worker_health_monitor</code></td>
                </tr>
                <tr>
                    <td>Can't SSH at all</td>
                    <td><code>ssh-keygen -t ed25519 -f ~/.ssh/id_ed25519 -N "" && ssh-copy-id airflow@45.151.155.74</code></td>
                </tr>
                <tr>
                    <td>Complete reset needed</td>
                    <td>See "Complete Reset" section below</td>
                </tr>
            </table>

            <h4>Complete Reset (Nuclear Option):</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Full reset and setup
ssh-keygen -t ed25519 -f ~/.ssh/id_ed25519 -N ""
ssh-copy-id airflow@45.151.155.74
ssh airflow@45.151.155.74 "hostname"
python scripts/manage_workers.py set-status worker-1 available
airflow dags unpause worker_health_monitor
python scripts/manage_workers.py list
            </div>
        </div>

        <!-- Issue 0: V2 Operator Issues -->
        <h3>üéØ Issue 0: Smart Remote Worker V2 Troubleshooting</h3>

        <div class="step" style="background: #e3f2fd; border-left: 4px solid #2196f3;">
            <h4>Common V2 Operator Issues:</h4>
            
            <h4>Problem 1: "airflow_home not set in worker config"</h4>
            <div class="warning-box">
                <strong>Error:</strong> <code>Worker worker-1 does not have airflow_home configured</code>
                <p><strong>Cause:</strong> The WORKER_REGISTRY doesn't have airflow_home field for the worker</p>
                <p><strong>Solution:</strong></p>
                <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Run the update script
python update_worker_config.py

# Or manually update
python -c "
from airflow.models import Variable
import json
registry = json.loads(Variable.get('WORKER_REGISTRY'))
registry['workers']['worker-1']['airflow_home'] = '/home/airflow/airflow-worker'
Variable.set('WORKER_REGISTRY', json.dumps(registry))
print('‚úÖ Updated')
"

# Verify
python scripts/manage_workers.py show worker-1 | grep AIRFLOW_HOME
                </div>
            </div>

            <h4>Problem 2: "SSH connection failed"</h4>
            <div class="warning-box">
                <strong>Error:</strong> <code>Failed to establish SSH connection to worker-1</code>
                <p><strong>Cause:</strong> SSH keys not set up or connection failed</p>
                <p><strong>Solution:</strong></p>
                <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Test SSH manually first
ssh airflow@45.151.155.74 "hostname"

# If fails, set up SSH keys
ssh-copy-id airflow@45.151.155.74

# If connection exists in Airflow, delete and let V2 recreate
airflow connections delete worker_worker-1

# Retry the task - V2 will auto-create the connection
                </div>
            </div>

            <h4>Problem 3: "Singularity image not found"</h4>
            <div class="warning-box">
                <strong>Warning:</strong> <code>Singularity image not found, executing directly</code>
                <p><strong>Cause:</strong> Singularity .sif file doesn't exist or in wrong location</p>
                <p><strong>Solution:</strong></p>
                <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Check on remote worker
ssh airflow@45.151.155.74 "ls -la /home/airflow/airflow-worker/containers/"

# Should see: airflow-worker.sif
# If missing, build it on remote worker:
ssh airflow@45.151.155.74 "cd /home/airflow/airflow-worker && \
  mkdir -p containers && \
  cd containers && \
  singularity build airflow-worker.sif docker://apache/airflow:2.10.2"
                </div>
                <p><strong>Note:</strong> V2 will still execute the command without Singularity, but you may want the container for consistency</p>
            </div>

            <h4>Problem 4: "Task fails but SSH works"</h4>
            <div class="warning-box">
                <strong>Symptom:</strong> SSH connection works, but task returns non-zero exit code
                <p><strong>Cause:</strong> Command itself is failing on remote worker</p>
                <p><strong>Debug:</strong></p>
                <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Test command directly on remote worker
ssh airflow@45.151.155.74 "cd /home/airflow/airflow-worker && YOUR_COMMAND"

# Check task logs for exact error
cat ~/airflow/logs/dag_id=YOUR_DAG/run_id=*/task_id=YOUR_TASK/attempt=1.log | tail -100

# Look for the "üìÑ Output:" section to see actual command output
# Look for "üìä Exit status:" - non-zero means command failed
                </div>
                <p><strong>Common causes:</strong></p>
                <ul>
                    <li>Python script not found (check path)</li>
                    <li>Missing dependencies in Singularity image</li>
                    <li>Wrong working directory</li>
                    <li>Permission issues</li>
                </ul>
            </div>

            <h4>Problem 5: "Worker unavailable" even though SSH works</h4>
            <div class="warning-box">
                <strong>Symptom:</strong> <code>No available workers matching priority</code>
                <p><strong>Cause:</strong> Worker status is set to 'unavailable' or 'maintenance' in registry</p>
                <p><strong>Solution:</strong></p>
                <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Check current status
python scripts/manage_workers.py show worker-1

# Update to available
python scripts/manage_workers.py set-status worker-1 available

# Verify
python scripts/manage_workers.py list
                </div>
            </div>

            <h4>Problem 6: "Different AIRFLOW_HOME on master vs worker"</h4>
            <div class="info-box">
                <strong>This is NORMAL and CORRECT!</strong>
                <p>Master and remote workers can (and should!) have different AIRFLOW_HOME paths:</p>
                <ul>
                    <li>Master: <code>/home/airflow/airflow</code> ‚Üí Manages orchestration, scheduler, webserver</li>
                    <li>Worker: <code>/home/airflow/airflow-worker</code> ‚Üí Executes tasks only</li>
                </ul>
                <p>The WORKER_REGISTRY stores the <strong>remote worker's path</strong>, not the master's!</p>
                <p><strong>No action needed</strong> - this is by design!</p>
            </div>
        </div>

        <h3>üîç V2 Operator Diagnostic Checklist</h3>

        <div class="step">
            <h4>Run these commands in order to diagnose V2 issues:</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
echo "========== V2 OPERATOR DIAGNOSTIC =========="

echo ""
echo "1. Check worker registry configuration:"
python scripts/manage_workers.py show worker-1

echo ""
echo "2. Verify SSH connection:"
ssh airflow@45.151.155.74 "hostname && pwd && ls -la"

echo ""
echo "3. Check remote AIRFLOW_HOME:"
ssh airflow@45.151.155.74 "echo \$AIRFLOW_HOME && ls -la /home/airflow/airflow-worker/"

echo ""
echo "4. Check Singularity image:"
ssh airflow@45.151.155.74 "ls -lh /home/airflow/airflow-worker/containers/*.sif"

echo ""
echo "5. Test simple command:"
airflow dags trigger test_smart_v2

echo ""
echo "Wait 30 seconds for task to complete..."
sleep 30

echo ""
echo "6. Check task status:"
airflow dags list-runs -d test_smart_v2 -o table --num-runs 1

echo ""
echo "7. View task logs:"
cat ~/airflow/logs/dag_id=test_smart_v2/run_id=manual*/task_id=test_simple_command/attempt=1.log | tail -50

echo ""
echo "========== DIAGNOSTIC COMPLETE =========="
            </div>
            
            <p><strong>Expected output if everything works:</strong></p>
            <ul>
                <li>‚úÖ Worker shows: <code>AIRFLOW_HOME: /home/airflow/airflow-worker</code></li>
                <li>‚úÖ SSH returns: <code>hostname</code> and directory listing</li>
                <li>‚úÖ Singularity shows: <code>airflow-worker.sif</code> file exists</li>
                <li>‚úÖ Task status shows: <code>success</code></li>
                <li>‚úÖ Logs show: <code>[INFO] ‚úÖ SUCCESS: Task completed</code></li>
            </ul>
        </div>

        <!-- Diagnostic Script -->
        <h3>üîç Complete Worker Diagnostic Script</h3>

        <div class="step">
            <h4>Run This to Diagnose All Issues:</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
#!/bin/bash
echo "========================================="
echo "WORKER DIAGNOSTICS"
echo "========================================="

echo -e "\n1. Worker Registry Status:"
python scripts/manage_workers.py list

echo -e "\n2. SSH Connection Test:"
if ssh -o BatchMode=yes -o ConnectTimeout=5 airflow@45.151.155.74 "echo OK" 2>/dev/null; then
    echo "‚úÖ SSH works"
else
    echo "‚ùå SSH failed"
fi

echo -e "\n3. SSH Keys:"
ls -la ~/.ssh/id_* 2>/dev/null || echo "No SSH keys"

echo -e "\n4. Health Monitor:"
airflow dags show worker_health_monitor 2>/dev/null | grep "is_paused"

echo -e "\n5. Network Test:"
ping -c 2 45.151.155.74 2>/dev/null || echo "‚ùå Cannot ping worker"

echo -e "\n========================================="
            </div>

            <strong>Save as <code>diagnose.sh</code> and run:</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
chmod +x diagnose.sh
./diagnose.sh
            </div>
        </div>

        <h3>Issue 1: Worker Shows "Unavailable"</h3>
        <div class="step">
            <strong>Symptoms:</strong> Worker status in registry shows "unavailable"
            <br><strong>Possible Causes:</strong>
            <ul>
                <li>Worker machine is down</li>
                <li>SSH service not running on worker</li>
                <li>Network connectivity issue</li>
                <li>SSH key authentication failed</li>
            </ul>
            <strong>How to diagnose:</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Test SSH connection manually
ssh -i /home/airflow/.ssh/airflow_worker_key airflow@45.151.155.74 "echo OK"

# Check worker machine is reachable
ping 45.151.155.74

# Check SSH service on worker
ssh -i /home/airflow/.ssh/airflow_worker_key airflow@45.151.155.74 "systemctl status sshd"
            </div>
            <strong>Solution:</strong>
            <ul>
                <li>Restart SSH service on worker</li>
                <li>Verify SSH key permissions (600 for private key)</li>
                <li>Check firewall rules</li>
            </ul>
        </div>

        <h3>Issue 2: Task Fails with "No Available Workers"</h3>
        <div class="step">
            <strong>Symptoms:</strong> DAG execution fails with error "No workers available in priority list"
            <br><strong>Possible Causes:</strong>
            <ul>
                <li>All workers in priority list are unavailable</li>
                <li>Worker names in DAG don't match registry</li>
                <li>Registry not properly configured</li>
            </ul>
            <strong>How to diagnose:</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Check registry contents
airflow variables get WORKER_REGISTRY

# Check health monitor status
airflow dags list-runs -d worker_health_monitor

# Manually trigger health check
airflow dags trigger worker_health_monitor
            </div>
            <strong>Solution:</strong>
            <ul>
                <li>Verify worker names match in DAG and registry</li>
                <li>Check at least one worker is "available"</li>
                <li>Add fallback workers to priority list</li>
            </ul>
        </div>

        <h3>Issue 3: SSH Tunnel Connection Timeout</h3>
        <div class="step">
            <strong>Symptoms:</strong> Task logs show "SSH tunnel connection timeout" or "Could not connect to database"
            <br><strong>Possible Causes:</strong>
            <ul>
                <li>Firewall blocking SSH tunnel ports</li>
                <li>PostgreSQL/Broker not listening on correct ports</li>
                <li>SSH timeout too short</li>
            </ul>
            <strong>How to diagnose:</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Test tunnel manually
ssh -i /home/airflow/.ssh/airflow_worker_key \
    -L 15432:localhost:5432 \
    airflow@45.151.155.74 \
    "sleep 10"

# In another terminal, test PostgreSQL connection
psql -h localhost -p 15432 -U airflow -d airflow_db -c "SELECT 1;"
            </div>
            <strong>Solution:</strong>
            <ul>
                <li>Increase SSH timeout in operator configuration</li>
                <li>Verify PostgreSQL listening on 0.0.0.0 or localhost</li>
                <li>Check <code>pg_hba.conf</code> allows connections</li>
            </ul>
        </div>

        <h3>Issue 4: Worker Process Doesn't Stop After Task</h3>
        <div class="step">
            <strong>Symptoms:</strong> Celery worker process remains running on remote after task completes
            <br><strong>Possible Causes:</strong>
            <ul>
                <li>Worker launcher script not properly stopping worker</li>
                <li>Container not exiting cleanly</li>
                <li>SSH connection not closing</li>
            </ul>
            <strong>How to diagnose:</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Check running processes on worker
ssh -i /home/airflow/.ssh/airflow_worker_key airflow@45.151.155.74 \
    "ps aux | grep celery"

# Check Singularity processes
ssh -i /home/airflow/.ssh/airflow_worker_key airflow@45.151.155.74 \
    "ps aux | grep singularity"
            </div>
            <strong>Solution:</strong>
            <ul>
                <li>Update worker launcher script to properly stop containers</li>
                <li>Add timeout to worker execution</li>
                <li>Manually stop orphaned processes</li>
            </ul>
        </div>

        <!-- ===== SECURITY BEST PRACTICES ===== -->
        <h2>üîí Security Best Practices</h2>

        <div class="warning-box">
            <h3>SSH Key Management</h3>
            <ul>
                <li><strong>Never share private keys</strong> - Keep <code>airflow_worker_key</code> on master only</li>
                <li><strong>Use correct permissions:</strong>
                    <ul>
                        <li>Private key: <code>chmod 600</code></li>
                        <li>Public key: <code>chmod 644</code></li>
                        <li><code>.ssh</code> directory: <code>chmod 700</code></li>
                    </ul>
                </li>
                <li><strong>Rotate keys periodically</strong> (every 6-12 months)</li>
                <li><strong>Use separate keys per environment</strong> (dev, staging, prod)</li>
            </ul>
        </div>

        <div class="warning-box">
            <h3>Worker Registry Security</h3>
            <ul>
                <li><strong>Don't store passwords</strong> in registry (use SSH keys only)</li>
                <li><strong>Restrict Airflow Variables access</strong> to admin users only</li>
                <li><strong>Log registry changes</strong> for audit trail</li>
                <li><strong>Validate worker IPs</strong> before adding to registry</li>
            </ul>
        </div>

        <div class="warning-box">
            <h3>Network Security</h3>
            <ul>
                <li><strong>Firewall rules:</strong> Only allow SSH (port 22) from master IP to workers</li>
                <li><strong>No direct exposure:</strong> PostgreSQL and broker should NOT be accessible from workers without tunnel</li>
                <li><strong>Use VPN</strong> if workers are on different networks</li>
                <li><strong>Enable SSH logs</strong> on both master and workers for monitoring</li>
            </ul>
        </div>

        <!-- ===== SUMMARY ===== -->
        <h2>‚úÖ Setup Summary Checklist</h2>

        <table>
            <tr>
                <th>Step</th>
                <th>Action</th>
                <th>Verification</th>
            </tr>
            <tr>
                <td>1</td>
                <td>Master node prerequisites verified</td>
                <td><code>airflow version</code> shows CeleryExecutor</td>
            </tr>
            <tr>
                <td>2</td>
                <td>SSH keys generated on master</td>
                <td><code>ls ~/.ssh/airflow_worker_key*</code> shows both files</td>
            </tr>
            <tr>
                <td>3</td>
                <td>SSH access configured to worker</td>
                <td><code>ssh -i ~/.ssh/airflow_worker_key user@worker "echo OK"</code> returns OK</td>
            </tr>
            <tr>
                <td>4</td>
                <td>Worker launcher script deployed</td>
                <td><code>ssh worker "ls -l ~/worker_launcher.sh"</code> shows executable</td>
            </tr>
            <tr>
                <td>5</td>
                <td>SSH tunnel tested</td>
                <td>PostgreSQL connection through tunnel successful</td>
            </tr>
            <tr>
                <td>6</td>
                <td>Worker added to registry</td>
                <td><code>airflow variables get WORKER_REGISTRY</code> shows worker</td>
            </tr>
            <tr>
                <td>7</td>
                <td>Health monitor DAG active</td>
                <td><code>airflow dags list | grep worker_health_monitor</code> shows enabled</td>
            </tr>
            <tr>
                <td>8</td>
                <td>Example DAG tested</td>
                <td>Task executed successfully on remote worker</td>
            </tr>
        </table>

        <!-- ===== NEXT STEPS ===== -->
        <h2>üöÄ Next Steps After Setup</h2>

        <div class="step">
            <h3>1. Scale to Multiple Workers</h3>
            <ul>
                <li>Repeat setup steps for each new worker</li>
                <li>Add workers to registry with appropriate priorities</li>
                <li>Test with DAG that uses worker priority list</li>
            </ul>
        </div>

        <div class="step">
            <h3>2. Customize Worker Selection Logic</h3>
            <ul>
                <li>Use tags to categorize workers (gpu, high-memory, etc.)</li>
                <li>Implement custom selection algorithms in operator</li>
                <li>Add load balancing across workers</li>
            </ul>
        </div>

        <div class="step">
            <h3>3. Implement Monitoring</h3>
            <ul>
                <li>Set up alerts for worker unavailability</li>
                <li>Monitor SSH tunnel stability</li>
                <li>Track task execution times per worker</li>
                <li>Create dashboard showing worker health status</li>
            </ul>
        </div>

        <div class="step">
            <h3>4. Optimize Performance</h3>
            <ul>
                <li>Tune SSH timeout values</li>
                <li>Adjust health check frequency</li>
                <li>Configure worker resource limits</li>
                <li>Optimize container startup time</li>
            </ul>
        </div>

        <!-- ===== HOW WORKER LAUNCHER WORKS ===== -->
        <h2>üìñ Understanding worker_launcher.sh - How It Works</h2>

        <div class="info-box">
            <strong>Purpose:</strong> The <code>worker_launcher.sh</code> script is deployed to remote workers and handles the entire worker lifecycle: SSH tunneling, worker startup, task execution, and cleanup.
        </div>

        <h3>üéØ What Does worker_launcher.sh Do?</h3>
        
        <div class="step">
            <h4>Complete Workflow Overview</h4>
            <ol>
                <li><strong>Cleanup old processes</strong> - Removes any existing workers or tunnels</li>
                <li><strong>Create SSH tunnels</strong> - Establishes secure connection back to master</li>
                <li><strong>Start Celery worker</strong> - Launches worker in Singularity container</li>
                <li><strong>Process tasks</strong> - Executes DAG tasks assigned to this worker</li>
                <li><strong>Stop and cleanup</strong> - Shuts down worker and closes tunnels when done</li>
            </ol>
        </div>

        <h3>üîÑ Step-by-Step Execution Flow</h3>

        <div class="step" style="background: #fff3cd; border-left: 4px solid #ffc107;">
            <h4>Phase 1: Configuration</h4>
            <strong>What happens:</strong> Script reads environment variables passed from master
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Environment variables set by master
WORKER_NAME="worker-1"
MASTER_IP="192.168.1.100"  # Master node IP
QUEUE_NAME="default"
AIRFLOW_HOME="/home/airflow/airflow-worker/Airflow"
CONTAINER_TYPE="singularity"
            </div>
            <strong>Purpose:</strong> Master tells worker who it is and where to connect
        </div>

        <div class="step" style="background: #ffebee; border-left: 4px solid #f44336;">
            <h4>Phase 2: Cleanup Old Processes</h4>
            <strong>What happens:</strong> Kills any existing worker or SSH tunnel
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Stop old Celery worker
pkill -f "celery.*worker.*worker-1"

# Close old SSH tunnels
pkill -f "ssh.*-L.*5432"  # PostgreSQL tunnel
pkill -f "ssh.*-L.*6379"  # Redis tunnel
            </div>
            <strong>Purpose:</strong> Ensures clean state, no conflicts with previous runs
        </div>

        <div class="step" style="background: #e3f2fd; border-left: 4px solid #2196f3;">
            <h4>Phase 3: Setup SSH Tunnels (THE KEY FEATURE!)</h4>
            <strong>What happens:</strong> Creates reverse SSH tunnels from worker to master
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Create SSH tunnel from worker back to master
ssh -f -N \
    -L 5432:localhost:5432 \  # PostgreSQL tunnel
    -L 6379:localhost:6379 \  # Redis tunnel
    airflow@192.168.1.100     # Master IP
            </div>
            <strong>How it works:</strong>
            <div class="flow-diagram">
                <div style="background: #c8e6c9; padding: 15px; margin: 10px;">
                    <strong>Remote Worker</strong><br>
                    localhost:5432 (worker) ‚óÑ‚îÄ‚îÄ‚îÄ SSH Tunnel ‚îÄ‚îÄ‚îÄ‚ñ∫ Master PostgreSQL:5432<br>
                    localhost:6379 (worker) ‚óÑ‚îÄ‚îÄ‚îÄ SSH Tunnel ‚îÄ‚îÄ‚îÄ‚ñ∫ Master Redis:6379
                </div>
            </div>
            <strong>Result:</strong> Worker can connect to "localhost:5432" but it actually connects to master's database through secure SSH tunnel!
            <div class="info-box">
                <strong>Why SSH tunnels?</strong>
                <ul>
                    <li>Worker has NO direct network access to master's PostgreSQL/Redis</li>
                    <li>All traffic goes through encrypted SSH connection</li>
                    <li>Master's databases not exposed to network</li>
                    <li>Worker thinks it's connecting to localhost (but it's actually master!)</li>
                </ul>
            </div>
        </div>

        <div class="step" style="background: #e8f5e9; border-left: 4px solid #4caf50;">
            <h4>Phase 4: Start Celery Worker in Singularity</h4>
            <strong>What happens:</strong> Launches Celery worker inside Singularity container
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Check Singularity image exists
ls $AIRFLOW_HOME/airflow_2.10.2.sif

# Start worker in Singularity container
nohup singularity exec \
    --bind $AIRFLOW_HOME:$AIRFLOW_HOME \
    airflow_2.10.2.sif \
    celery worker \
    --hostname=worker-1 \
    --queues=default \
    --concurrency=4 \
    > /tmp/worker_worker-1.log 2>&1 &

# Save process ID
echo $! > /tmp/worker_worker-1.pid
            </div>
            <strong>What this means:</strong>
            <ul>
                <li><code>singularity exec</code> - Run command inside container</li>
                <li><code>--bind</code> - Mount Airflow directory inside container</li>
                <li><code>celery worker</code> - Start Celery worker process</li>
                <li><code>--hostname=worker-1</code> - Identifies this worker</li>
                <li><code>--queues=default</code> - Which task queues to listen to</li>
                <li><code>nohup ... &</code> - Run in background</li>
            </ul>
            <strong>Worker is now running and listening for tasks!</strong>
        </div>

        <div class="step" style="background: #f3e5f5; border-left: 4px solid #9c27b0;">
            <h4>Phase 5: Task Execution</h4>
            <strong>How tasks flow from master to worker:</strong>
            <div class="flow-diagram">
                <div style="background: #e1f5fe; padding: 10px; margin: 5px; border-radius: 5px;">
                    <strong>1. Master:</strong> Task queued to Redis
                </div>
                <div class="arrow">‚Üì (via SSH tunnel)</div>
                <div style="background: #c8e6c9; padding: 10px; margin: 5px; border-radius: 5px;">
                    <strong>2. Worker:</strong> Picks up task from Redis
                </div>
                <div class="arrow">‚Üì</div>
                <div style="background: #fff9c4; padding: 10px; margin: 5px; border-radius: 5px;">
                    <strong>3. Worker:</strong> Executes bash_command from DAG
                </div>
                <div class="arrow">‚Üì (via SSH tunnel)</div>
                <div style="background: #e1f5fe; padding: 10px; margin: 5px; border-radius: 5px;">
                    <strong>4. Master:</strong> Worker updates status in PostgreSQL
                </div>
                <div class="arrow">‚Üì</div>
                <div style="background: #c8e6c9; padding: 10px; margin: 5px; border-radius: 5px;">
                    <strong>5. Master:</strong> Task complete, visible in Airflow UI
                </div>
            </div>
        </div>

        <div class="step" style="background: #ffebee; border-left: 4px solid #ef5350;">
            <h4>Phase 6: Cleanup (On-Demand Shutdown)</h4>
            <strong>What happens:</strong> Worker stops after task completion
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Stop Celery worker
pkill -f "celery.*worker.*worker-1"

# Close SSH tunnels
pkill -f "ssh.*-L.*5432"
pkill -f "ssh.*-L.*6379"

# Worker is now stopped (on-demand model)
            </div>
            <strong>Result:</strong> No persistent processes on remote worker!
        </div>

        <h3>üîç Real Example: Complete Trace</h3>

        <div class="command">
<strong>Master triggers DAG task on remote worker...</strong>

<strong>Step 1: Master SSHs into worker and runs:</strong>
$ ssh worker-1 "export WORKER_NAME=worker-1; \
                export MASTER_IP=192.168.1.100; \
                /home/airflow/worker_launcher.sh"

<strong>Step 2: On worker machine:</strong>
[worker-1] $ pkill celery          # Cleanup
[worker-1] $ ssh -f -N -L 5432:localhost:5432 master  # Create tunnels
[worker-1] $ singularity exec airflow_2.10.2.sif celery worker &  # Start
[worker-1] Worker started, listening for tasks...

<strong>Step 3: Worker connects to master (via tunnels):</strong>
[worker-1] Connecting to postgresql://localhost:5432  ‚Üê Actually master!
[worker-1] Connecting to redis://localhost:6379       ‚Üê Actually master!

<strong>Step 4: Worker picks up and executes task:</strong>
[worker-1] Received task: my_dag.my_task
[worker-1] Executing: echo "Hello from worker"
[worker-1] Task completed successfully

<strong>Step 5: Master calls cleanup:</strong>
$ ssh worker-1 "pkill celery; pkill ssh"
[worker-1] Worker stopped, tunnels closed
        </div>

        <h3>üéØ Key Benefits</h3>

        <table>
            <tr>
                <th>Feature</th>
                <th>How It Works</th>
                <th>Benefit</th>
            </tr>
            <tr>
                <td><strong>SSH Tunnels</strong></td>
                <td>Worker connects to localhost, traffic goes through SSH to master</td>
                <td>‚úÖ Secure, no direct database access needed</td>
            </tr>
            <tr>
                <td><strong>On-Demand</strong></td>
                <td>Worker starts when task arrives, stops after completion</td>
                <td>‚úÖ No persistent processes, saves resources</td>
            </tr>
            <tr>
                <td><strong>Singularity</strong></td>
                <td>Worker runs inside container image</td>
                <td>‚úÖ No root access needed, consistent environment</td>
            </tr>
            <tr>
                <td><strong>Cleanup</strong></td>
                <td>Script traps signals and always cleans up</td>
                <td>‚úÖ No orphaned processes or tunnels</td>
            </tr>
        </table>

        <h3>üîß Script Configuration Variables</h3>

        <table>
            <tr>
                <th>Variable</th>
                <th>Default</th>
                <th>Description</th>
            </tr>
            <tr>
                <td><code>WORKER_NAME</code></td>
                <td>worker-default</td>
                <td>Unique identifier for this worker</td>
            </tr>
            <tr>
                <td><code>MASTER_IP</code></td>
                <td>localhost</td>
                <td>Master node IP address</td>
            </tr>
            <tr>
                <td><code>QUEUE_NAME</code></td>
                <td>default</td>
                <td>Celery queue to listen to</td>
            </tr>
            <tr>
                <td><code>AIRFLOW_HOME</code></td>
                <td>/home/airflow/airflow</td>
                <td>Where Singularity image is located</td>
            </tr>
            <tr>
                <td><code>CONTAINER_TYPE</code></td>
                <td>singularity</td>
                <td>Container type (singularity or venv)</td>
            </tr>
            <tr>
                <td><code>CONCURRENCY</code></td>
                <td>4</td>
                <td>Number of parallel tasks</td>
            </tr>
        </table>

        <h3>üìä Worker Lifecycle Diagram</h3>

        <div class="flow-diagram">
            <div style="background: #fff9c4; padding: 15px; margin: 10px; border-radius: 5px;">
                <strong>IDLE STATE</strong><br>
                No worker running on remote machine
            </div>
            <div class="arrow">‚Üì (DAG triggers)</div>
            <div style="background: #e3f2fd; padding: 15px; margin: 10px; border-radius: 5px;">
                <strong>STARTING</strong><br>
                Master calls worker_launcher.sh via SSH
            </div>
            <div class="arrow">‚Üì</div>
            <div style="background: #c8e6c9; padding: 15px; margin: 10px; border-radius: 5px;">
                <strong>RUNNING</strong><br>
                Worker processes tasks (tunneled to master)
            </div>
            <div class="arrow">‚Üì (Task complete)</div>
            <div style="background: #ffebee; padding: 15px; margin: 10px; border-radius: 5px;">
                <strong>STOPPING</strong><br>
                Master calls cleanup, worker shuts down
            </div>
            <div class="arrow">‚Üì</div>
            <div style="background: #fff9c4; padding: 15px; margin: 10px; border-radius: 5px;">
                <strong>IDLE STATE</strong><br>
                Back to idle, ready for next task
            </div>
        </div>

        <div class="warning-box">
            <h3>‚ö†Ô∏è Important Notes</h3>
            <ul>
                <li><strong>Script Location:</strong> Must be in worker's home directory: <code>~/worker_launcher.sh</code></li>
                <li><strong>Executable:</strong> Must have execute permissions: <code>chmod +x worker_launcher.sh</code></li>
                <li><strong>AIRFLOW_HOME:</strong> Must be set on worker and point to where Singularity image is</li>
                <li><strong>SSH Keys:</strong> Master must have passwordless SSH access to worker</li>
                <li><strong>Singularity Image:</strong> Must be copied to <code>$AIRFLOW_HOME/airflow_2.10.2.sif</code></li>
            </ul>
        </div>

        <h3>üîç Debugging Worker Issues</h3>

        <div class="step">
            <h4>Check if worker is running:</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# On remote worker
ps aux | grep celery | grep worker-1

# Check worker log
tail -f /tmp/worker_worker-1.log
            </div>
        </div>

        <div class="step">
            <h4>Check if SSH tunnels are active:</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# On remote worker
netstat -tuln | grep 5432  # PostgreSQL tunnel
netstat -tuln | grep 6379  # Redis tunnel

# Or check SSH processes
ps aux | grep "ssh.*-L"
            </div>
        </div>

        <div class="step">
            <h4>Test tunnel connectivity:</h4>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# On remote worker (through tunnel)
psql -h localhost -p 5432 -U airflow -d airflow_db -c "SELECT 1;"

# Test Redis
redis-cli -h localhost -p 6379 ping
            </div>
        </div>

        <!-- ===== VIEW WORKERS ===== -->
        <h2>üîç How to View Worker/Remote Server List</h2>

        <div class="info-box">
            <strong>Workers are stored in Airflow Variables</strong> under the key <code>WORKER_REGISTRY</code>. You can view them in multiple ways.
        </div>

        <h3>Method 1: CLI Tool (Recommended)</h3>
        <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# List all workers with formatted table
python scripts/manage_workers.py list

# Show detailed info for specific worker
python scripts/manage_workers.py show worker-1
        </div>

        <strong>Output Example:</strong>
        <div class="command">
==================================================================================
üìã WORKER REGISTRY
==================================================================================
Last updated: 2024-10-25T10:30:00

Worker ID            Host                 Status       Priority Container    User      
------------------------------------------------------------------------------------------
‚úÖ worker-1          45.151.155.74        available    1        singularity  airflow   
‚ùå worker-2          10.0.1.20            unavailable  2        singularity  airflow   
==================================================================================
Total workers: 2
Available: 1
Unavailable: 1
        </div>

        <h3>Method 2: Airflow Web UI (Visual)</h3>
        <ol>
            <li>Open Airflow Web UI: <code>http://localhost:8080</code></li>
            <li>Click <strong>Admin</strong> in top menu</li>
            <li>Click <strong>Variables</strong></li>
            <li>Search for: <code>WORKER_REGISTRY</code></li>
            <li>Click on the row to view JSON</li>
        </ol>
        <div class="info-box">
            <strong>UI Navigation:</strong> Top Menu ‚Üí Admin ‚Üí Variables ‚Üí WORKER_REGISTRY
        </div>

        <h3>Method 3: Airflow CLI</h3>
        <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Get raw JSON
airflow variables get WORKER_REGISTRY

# Pretty print JSON
airflow variables get WORKER_REGISTRY | python -m json.tool

# Export to file
airflow variables get WORKER_REGISTRY > workers.json
        </div>

        <h3>Method 4: Health Monitor Logs</h3>
        <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# View latest health check logs
airflow dags list-runs -d worker_health_monitor --state success -n 1

# View specific run logs (replace date)
airflow tasks logs worker_health_monitor check_worker_health 2024-10-25
        </div>

        <h3>Quick Status Check</h3>
        <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Quick status of all workers
airflow variables get WORKER_REGISTRY | \
    python -c "import sys, json; data=json.load(sys.stdin); \
    [print(f'{k}: {v[\"status\"]}') for k,v in data['workers'].items()]"

# Count available workers
airflow variables get WORKER_REGISTRY | \
    python -c "import sys, json; data=json.load(sys.stdin); \
    print(f'Available: {sum(1 for w in data[\"workers\"].values() if w[\"status\"]==\"available\")}')"
        </div>

        <h3>Watch Mode (Real-time Monitoring)</h3>
        <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Watch worker status (updates every 2 seconds)
watch -n 2 'python scripts/manage_workers.py list'
        </div>

        <div class="warning-box">
            <h3>Troubleshooting</h3>
            <strong>"WORKER_REGISTRY not found"</strong>
            <div class="command"><button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
# Check if variable exists
airflow variables list | grep WORKER_REGISTRY

# If not found, add your first worker
python scripts/manage_workers.py add worker-1 45.151.155.74 --user airflow
            </div>
        </div>

        <table>
            <tr>
                <th>Method</th>
                <th>Command</th>
                <th>Best For</th>
            </tr>
            <tr>
                <td>CLI Tool</td>
                <td><code>python scripts/manage_workers.py list</code></td>
                <td>Quick checks, formatted output</td>
            </tr>
            <tr>
                <td>Web UI</td>
                <td>Admin ‚Üí Variables ‚Üí WORKER_REGISTRY</td>
                <td>Visual management, editing</td>
            </tr>
            <tr>
                <td>Airflow CLI</td>
                <td><code>airflow variables get WORKER_REGISTRY</code></td>
                <td>Scripting, automation</td>
            </tr>
            <tr>
                <td>Health Monitor</td>
                <td><code>airflow tasks logs worker_health_monitor ...</code></td>
                <td>Historical status, monitoring</td>
            </tr>
        </table>

        <!-- ===== REFERENCE ===== -->
        <h2>üìö Quick Reference</h2>

        <h3>Important File Locations</h3>
        <table>
            <tr>
                <th>Component</th>
                <th>Location</th>
            </tr>
            <tr>
                <td>SSH Private Key</td>
                <td><code>/home/airflow/.ssh/airflow_worker_key</code></td>
            </tr>
            <tr>
                <td>Worker Registry</td>
                <td>Airflow Variables ‚Üí <code>WORKER_REGISTRY</code></td>
            </tr>
            <tr>
                <td>Main Operator</td>
                <td><code>/home/airflow/airflow/plugins/operators/smart_remote_worker.py</code></td>
            </tr>
            <tr>
                <td>Health Monitor DAG</td>
                <td><code>/home/airflow/airflow/dags/worker_health_monitor.py</code></td>
            </tr>
            <tr>
                <td>Management CLI</td>
                <td><code>/home/airflow/airflow/scripts/manage_workers.py</code></td>
            </tr>
            <tr>
                <td>Worker Launcher</td>
                <td><code>/home/airflow/worker_launcher.sh</code> (on remote worker)</td>
            </tr>
        </table>

        <h3>Common Commands</h3>
        <table>
            <tr>
                <th>Task</th>
                <th>Command</th>
            </tr>
            <tr>
                <td>List workers</td>
                <td><code>python scripts/manage_workers.py list</code></td>
            </tr>
            <tr>
                <td>Add worker</td>
                <td><code>python scripts/manage_workers.py add WORKER_ID IP --user USER</code></td>
            </tr>
            <tr>
                <td>Remove worker</td>
                <td><code>python scripts/manage_workers.py remove WORKER_ID</code></td>
            </tr>
            <tr>
                <td>Test SSH</td>
                <td><code>ssh -i ~/.ssh/airflow_worker_key USER@IP "hostname"</code></td>
            </tr>
            <tr>
                <td>Check health</td>
                <td><code>airflow dags trigger worker_health_monitor</code></td>
            </tr>
            <tr>
                <td>View registry</td>
                <td><code>airflow variables get WORKER_REGISTRY</code></td>
            </tr>
        </table>

        <div class="info-box">
            <h3>üéØ Key Takeaways</h3>
            <ul>
                <li><span class="highlight">No hardcoded IPs</span> - All workers managed via Airflow Variables</li>
                <li><span class="highlight">Easy scaling</span> - Add workers without touching code</li>
                <li><span class="highlight">Priority selection</span> - Each DAG chooses preferred workers</li>
                <li><span class="highlight">On-demand</span> - Workers start only when needed</li>
                <li><span class="highlight">SSH-only</span> - All communication through secure tunnels</li>
                <li><span class="highlight">Auto-retry</span> - 5-minute wait with automatic failover</li>
                <li><span class="highlight">No root</span> - Everything runs as regular user</li>
            </ul>
        </div>

        <hr>
        <p style="text-align: center; color: #666; margin-top: 30px;">
            <strong>Document Version:</strong> 1.0 | <strong>Last Updated:</strong> 2024-10-25
            <br>For questions or issues, refer to troubleshooting section or contact admin.
        </p>
    </div>
</body>
</html>
